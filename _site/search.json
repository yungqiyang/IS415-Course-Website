[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IS415 Course Website",
    "section": "",
    "text": "Hello everybody, my name is Qi Yang and this is my progress through IS415 - Geospatial Analytics and Applications!"
  },
  {
    "objectID": "Take-Home_Exercise/Take-Home_Exercise_01/Take-Home_Exercise_01.html",
    "href": "Take-Home_Exercise/Take-Home_Exercise_01/Take-Home_Exercise_01.html",
    "title": "Take-Home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab’s hailing services in Singapore",
    "section": "",
    "text": "The following R-packages and their use case are as follows:\n\narrow: The Grab-Posisi datasets are stored in snappy.parquet format, so we will need to leverage upon the arrow package to read and write these data.\ntidyverse: To allow us to investigate, manipulate and write the datasets of interest in a readable manner. Also has great compatibility with sf objects.\nlubridate: To handle time format data and conversion of other data types into time formats.\nsf: Allows us to handle spatial data, specifically the master plan subzone and OpenStreetMap road network data.\nspatstat: Used to compute the Clark-Evan’s test and various Kernel Density Estimates in this report.\nmaptools: Dependency for spatstat.\ntmap: For visualisation of the spatial data.\nhtmlwidgets: Used to save interactive spatial data visualisations as HTML widgets to lessen the burden on Quarto when rendering.\n\nThe p_load() functionfrom the pacman package is used to load the libraries and automatically install them for the user if they aren’t already installed.\n\n\nReveal Code\npacman::p_load(sf, tidyverse, arrow, lubridate, tmap, maptools, spatstat, spNetwork, htmlwidgets)"
  },
  {
    "objectID": "In-Class_Exercise/In-Class_Exercise_05/In-Class_Exercise_05.html",
    "href": "In-Class_Exercise/In-Class_Exercise_05/In-Class_Exercise_05.html",
    "title": "In-Class Exercise 5: Global and Local Measures of Spatial Autocorrelation using sfDep methods",
    "section": "",
    "text": "Reveal Code\npacman::p_load(sf, sfdep, plotly, tidyverse, tmap, Kendall)"
  },
  {
    "objectID": "Take-Home_Exercise/Take-Home_Exercise_01/Take-Home_Exercise_01.html#preparing-the-r-environment-for-analysis",
    "href": "Take-Home_Exercise/Take-Home_Exercise_01/Take-Home_Exercise_01.html#preparing-the-r-environment-for-analysis",
    "title": "Take-Home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab’s hailing services in Singapore",
    "section": "",
    "text": "The following R-packages and their use case are as follows:\n\narrow: The Grab-Posisi datasets are stored in snappy.parquet format, so we will need to leverage upon the arrow package to read and write these data.\ntidyverse: To allow us to investigate, manipulate and write the datasets of interest in a readable manner. Also has great compatibility with sf objects.\nlubridate: To handle time format data and conversion of other data types into time formats.\nsf: Allows us to handle spatial data, specifically the master plan subzone and OpenStreetMap road network data.\nspatstat: Used to compute the Clark-Evan’s test and various Kernel Density Estimates in this report.\nmaptools: Dependency for spatstat.\ntmap: For visualisation of the spatial data.\nhtmlwidgets: Used to save interactive spatial data visualisations as HTML widgets to lessen the burden on Quarto when rendering.\n\nThe p_load() functionfrom the pacman package is used to load the libraries and automatically install them for the user if they aren’t already installed.\n\n\nReveal Code\npacman::p_load(sf, tidyverse, arrow, lubridate, tmap, maptools, spatstat, spNetwork, htmlwidgets)"
  },
  {
    "objectID": "Take-Home_Exercise/Take-Home_Exercise_01/Take-Home_Exercise_01.html#conducting-data-pre-processing-for-analysis",
    "href": "Take-Home_Exercise/Take-Home_Exercise_01/Take-Home_Exercise_01.html#conducting-data-pre-processing-for-analysis",
    "title": "Take-Home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab’s hailing services in Singapore",
    "section": "Conducting data pre-processing for analysis",
    "text": "Conducting data pre-processing for analysis\n\nImporting the datasets of interest\nFor this report, 3 different datasets are of interest. Grab-Posisi (parquet) from Grab to obtain the spatial events of Grab trips, MPSZ2019 (shp)from data.gov.sg to provide us with the geographical boundaries for our analysis and gis_osm_roads_free_1 (shp) from OpenStreetMap to obtain the road networks for analysis.\n\nGrab-Posisi Data:\nThe Grab-Posisi data is contained within 10 compartmentalised parquet files, thus, the read_parquet() function from the Arrow package is required. The files are loaded iteratively in a loop and immediately appended to each other without assignment using the bind_rows() function.\n\n\nReveal Code\ngrab_df &lt;- read_parquet(\"Data/Geospatial/GrabPosisi/part-00000-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\n\nfor (i in 1:9) {\n  grab_df &lt;- grab_df %&gt;%\n    bind_rows(read_parquet(paste0(\"Data/Geospatial/GrabPosisi/part-0000\", as.character(i), \"-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")))\n}\n\n\nWe can see that the Grab-Posisi dataset in its entirety at 30+ million rows is way too large for computation.\n\n\nReveal Code\nglimpse(grab_df)\n\ngrab_df$pingtimestamp &lt;- as_datetime(grab_df$pingtimestamp)\n\n\n\nWe will store the compiled Grab-Posisi just in case there is a need for its entirety in the future as there is spare memory on my drive for now.\n\n\nReveal Code\nwrite_parquet(grab_df, \"Data/Geospatial/GrabPosisi/Grab_Posisi.parquet\")\n\n\nLuckily, as we are interested only in the analysis of the origins of Grab calls in Singapore, the Grab-Posisi data can be subset to reduce its size in the R-Environment to the necessary only. This is achieved using the group_by() function to sort each trip record by its ping time-stamp within the trip itself only. filter() is used to extract only the earliest, and therefore, the origin record while mutate() is used to extract the day of the week in which the trip occurred. The result is stored as an R-Data Structure (RDS) file using the write_rds() function to avoid re-computation in the future.\n\n\nReveal Code\ngrab_origin_df &lt;- grab_df %&gt;% \n  group_by(trj_id) %&gt;% \n  arrange(pingtimestamp) %&gt;% \n  filter(row_number() == 1) %&gt;% \n  mutate(weekday = wday(pingtimestamp, label = TRUE, abbr = TRUE))\n\nglimpse(grab_origin_df)\n\npaste(\"From:\", min(grab_origin_df$pingtimestamp), \"To:\", max(grab_origin_df$pingtimestamp))\n\nwrite_rds(grab_origin_df, \"Data/Geospatial/RDS/Grab_Origins_Posisi.rds\")\n\n\n\nFrom a glimpse() of the subset data, we can see that the data set has been reduced considerably to 28,000 rows. This is still too large for kernel density estimations for my computer to handle, but for data wrangling and investigation purposes, it is good enough. As such, we will not rush to remove columns in case there is a need for them later especially as the main constraint in spatial computations is the number of events in the data.\n\nAdditionally, we can see that the data exists for 14 days only between 8th April 2019 and 21st April 2019.\nWe save this subset data so that the code may be re-run in the future without going through the costly process of compiling and subsetting again.\n\n\nReveal Code\ngrab_origin_df &lt;- read_rds(\"Data/Geospatial/RDS/Grab_Origins_Posisi.rds\")\n\n\n\n\nSingapore Master Plan Subzone and Road Network Data\nBoth the Singapore Master Plan Subzone (mpsz) and Road Network Data are spatial data which require the sf package to read in order to retain the spatial information contained. As such, we will use the st_read() function to read the data, and apply the st_transform() function to cast them to a consistent coordinate reference system (CRS) so that we may apply spatial analysis across both data. The CRS we will use is SVY21, which is specific for Singapore.\nThe Singapore Master Plan Subzone is loaded into the R-Environment below:\n\n\nReveal Code\nmpsz2019 &lt;- \n  st_read(\"Data/Geospatial/MPSZ-2019\", layer = \"MPSZ-2019\") %&gt;% \n  st_transform(crs = 3414)\n\n\nReading layer `MPSZ-2019' from data source \n  `C:\\Users\\yungq\\Desktop\\SMU Modules\\Y4S1\\Geospatial Analysis and Applications\\IS415 Course Website\\Take-Home_Exercise\\Take-Home_Exercise_01\\Data\\Geospatial\\MPSZ-2019' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\nReveal Code\nsummary(mpsz2019)\n\n\n  SUBZONE_N          SUBZONE_C          PLN_AREA_N         PLN_AREA_C       \n Length:332         Length:332         Length:332         Length:332        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n   REGION_N           REGION_C                  geometry  \n Length:332         Length:332         MULTIPOLYGON :332  \n Class :character   Class :character   epsg:3414    :  0  \n Mode  :character   Mode  :character   +proj=tmer...:  0  \n\n\nFrom the summary() of the data, we can see that it contains spatial polygon features, originally projected in WGS84, and we were successful in transforming the CRS to SVY21 (or EPSG: 3414 equivalently).\nPlotting the Master-Plan Subzone geospatial data with tmap’s tm_polygons() function for visualisation:\n\n\nReveal Code\nmpsz2019_plot &lt;- mpsz2019 %&gt;%\n  tm_shape() +\n  tm_polygons(col = \"lightyellow\", border.col = \"navy\") +\n  tm_layout(main.title = \"Singapore Master Plan Boundaries 2019\", main.title.size = 1) + tm_compass(size = 1)\n\ntmap_save(tm = mpsz2019_plot, \"Screenshots/mpsz2019.png\")\n\n\n\nThe OpenStreetMap (OSM) Road is loaded into the R-Environment below:\n\n\nReveal Code\nroads &lt;-\n  st_read(\"Data/Geospatial/OSM\", layer = \"gis_osm_roads_free_1\") %&gt;%\n  st_transform(crs = 3414)\n\nsummary(roads)\n\n\n\nHere, the data contains spatial linestrings, and like the mpsz data, is originally projected using WGS84 and successfully converted to SVY21. Also, although we know from the data source that the OSM road networks encompasses Singapore, Brunei and Malaysia, we can also tell from the R-outputs that it extends beyond Singapore’s borders. Compared to the mpsz data, its bounding box is clearly further extending. This also explains the vast number of features the data contains and its long reading time.\nIn the interest of computation times, we will subset the road networks to Singapore’s borders only. To do so, first we generate the Singaporean borders as a spatial polygon object using the st_union() function. We then employ the st_intersection() function on the generated outline of Singapore and the OSM road network to obtain the Singaporean road network only.\nFirst, the coastal outline of Singapore is generated, and its result is saved as an RDS file with the write_rds() function to again avoid re-generation in the future.\n\n\nReveal Code\ncoastal_outline &lt;- st_union(mpsz2019)\n\nwrite_rds(coastal_outline, \"Data/Geospatial/RDS/coastal_outline_sg.rds\")\n\n\n\n\nReveal Code\ncoastal_outline &lt;- read_rds(\"Data/Geospatial/RDS/coastal_outline_sg.rds\")\n\n\n\n\nReveal Code\nsg_coastal_outline_plot &lt;- tm_shape(coastal_outline) +\n  tm_polygons(col = \"white\", border.col = \"red\", lwd = 1.7) +\n  tm_layout(\"Coastal Outine of Singapore 2019\")\n\ntmap_save(sg_coastal_outline_plot, \"Screenshots/sg_coastal_outline_plot.png\")\n\n\n\nNext, we subset the OSM road networks by using the st_intersection() function to constrict the spatial line objects to those within the coastal boundaries of Singapore as generated. Considering the long computation time due to the size of the OSM data, we will save the constricted road network as an RDS file.\n\n\nReveal Code\nroads_sg &lt;- st_intersection(coastal_outline, roads)\n\nwrite_rds(roads_sg, \"Data/Geospatial/RDS/sg_road_network.rds\")\n\n\n\n\nReveal Code\nroads_sg &lt;- read_rds(\"Data/Geospatial/RDS/sg_road_network.rds\")\n\n\nVisualising the generated Singapore road networks with tmap using\n\n\nReveal Code\nmpsz_w_roads2019_plot &lt;-\n  tm_shape(coastal_outline) +\n  tm_polygons(col = \"lightyellow\", border.col = \"red\", border.alpha = 0.6) + tm_layout(main.title = \"Singapore Road Network 2019\", main.title.size = 1) +\n  tm_compass(size = 1) +\n  tm_shape(roads_sg) +\n  tm_lines()\n\ntmap_save(tm = mpsz_w_roads2019_plot, \"Screenshots/mpsz_w_roads2019.png\")\n\n\n\nHere we can observe that there is an extensive road network present outside the mainland of Singapore in the dataset. Thus, we will need to remove them if mainland Singapore is the scope of the study, or if they fall within the scope regardless as they will otherwise affect network contrained kernel density estimates. This is as Grab is unlikely to operate outside the mainland especially on islands like Pulau Tekong, thus, impacting global kernel density estimates because of its low or zero local density.\nFirst though, we should investigate the Grab-Posisi Data further as seen prior, we will need to reduce the scope of our analysis so that computations may be completed in a reasonable amount of time.\n\n\nTransforming the Grab-Posisi Data into a simple features object\nTo investigate the Grab-Posisi data, we must first cast it into a simple features object so that R can recognise the coordinate fields in the dataset as spatial and not simply numeric data. The st_as_af() function is used for this purpose, specifying the longitude and latitudes in that order, and the CRS as WGS84 (or EPSG: 4326 equivalently) as the data are geographical coordinates. The CRS of the data is then transformed into SVY21 format using the st_transform() function by specifying the crs parameter to be 3414, the EPSG code for SVY21.\n\n\nReveal Code\ngrab_origin_sf &lt;- grab_origin_df %&gt;%\n  st_as_sf(coords = c(\"rawlng\", \"rawlat\"), crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\nsummary(grab_origin_sf)\n\n\n    trj_id          driving_mode          osname         \n Length:28000       Length:28000       Length:28000      \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n pingtimestamp                        speed           bearing     \n Min.   :2019-04-08 00:09:26.00   Min.   :-1.000   Min.   :  0.0  \n 1st Qu.:2019-04-11 08:48:29.25   1st Qu.: 3.590   1st Qu.: 90.0  \n Median :2019-04-15 00:08:48.00   Median : 9.945   Median :179.0  \n Mean   :2019-04-14 21:29:59.93   Mean   : 9.566   Mean   :172.5  \n 3rd Qu.:2019-04-18 10:47:59.25   3rd Qu.:14.550   3rd Qu.:256.0  \n Max.   :2019-04-21 23:33:28.00   Max.   :30.949   Max.   :359.0  \n                                                                  \n    accuracy       weekday             geometry    \n Min.   :  1.000   Sun:3983   POINT        :28000  \n 1st Qu.:  3.900   Mon:3975   epsg:3414    :    0  \n Median :  6.000   Tue:4008   +proj=tmer...:    0  \n Mean   :  7.617   Wed:4016                        \n 3rd Qu.: 10.000   Thu:4008                        \n Max.   :728.000   Fri:4002                        \n                   Sat:4008                        \n\n\nWe can see from a the summary of the data that the casting of the grab origins data to a simple features object is successful and that a spatial point geometry column has been added to the data in place of “rawlng” and “rawlat”.\n\n\nTransforming the Grab-Posisi data further into Spatial Points, generic sp and ppp objects\nAs we are using the spatstat package, we will need to cast our Grab trip origins events data from a simple features (sf) object to a planar point pattern (ppp) object to work with it.\nTo do so, we can utilise the as_Spatial() function from the sf package to first convert it into a spatial point dataframe which can then be further transformed to a generic sp object using the as() from the same package by specifying the object class to be “SpatialPoints”. This is required to safely convert an sf object to a ppp object as the as() function does not support casting an sf object to a ppp object directly. The resulting sp object is then computed as a ppp object using the as() function but this time specifying the new object class to be “ppp”.\nThe transformation process and a summary of each casting step are as follows:\n\n\nReveal Code\ngrab_origin_spatial &lt;- grab_origin_sf %&gt;%\n  as_Spatial()\n\nsummary(grab_origin_spatial)\n\n\nObject of class SpatialPointsDataFrame\nCoordinates:\n                min      max\ncoords.x1  3628.243 49845.23\ncoords.x2 25198.140 49689.64\nIs projected: TRUE \nproj4string :\n[+proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1\n+x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0\n+units=m +no_defs]\nNumber of points: 28000\nData attributes:\n    trj_id          driving_mode          osname         \n Length:28000       Length:28000       Length:28000      \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n pingtimestamp                        speed           bearing     \n Min.   :2019-04-08 00:09:26.00   Min.   :-1.000   Min.   :  0.0  \n 1st Qu.:2019-04-11 08:48:29.25   1st Qu.: 3.590   1st Qu.: 90.0  \n Median :2019-04-15 00:08:48.00   Median : 9.945   Median :179.0  \n Mean   :2019-04-14 21:29:59.93   Mean   : 9.566   Mean   :172.5  \n 3rd Qu.:2019-04-18 10:47:59.25   3rd Qu.:14.550   3rd Qu.:256.0  \n Max.   :2019-04-21 23:33:28.00   Max.   :30.949   Max.   :359.0  \n                                                                  \n    accuracy       weekday   \n Min.   :  1.000   Sun:3983  \n 1st Qu.:  3.900   Mon:3975  \n Median :  6.000   Tue:4008  \n Mean   :  7.617   Wed:4016  \n 3rd Qu.: 10.000   Thu:4008  \n Max.   :728.000   Fri:4002  \n                   Sat:4008  \n\n\nReveal Code\ngrab_origin_sp &lt;- grab_origin_spatial %&gt;%\n  as(\"SpatialPoints\")\n\nsummary(grab_origin_sp)\n\n\nObject of class SpatialPoints\nCoordinates:\n                min      max\ncoords.x1  3628.243 49845.23\ncoords.x2 25198.140 49689.64\nIs projected: TRUE \nproj4string :\n[+proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1\n+x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0\n+units=m +no_defs]\nNumber of points: 28000\n\n\nReveal Code\ngrab_origin_ppp &lt;- grab_origin_sp %&gt;%  \n  as(\"ppp\")\n\nsummary(grab_origin_ppp)\n\n\nPlanar point pattern:  28000 points\nAverage intensity 2.473666e-05 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: rectangle = [3628.24, 49845.23] x [25198.14, 49689.64] units\n                    (46220 x 24490 units)\nWindow area = 1131920000 square units\n\n\n\n\n\nVisualising the Grab-Posisi trip origins data\nTo visualise the Grab trip origins data we prepared, we can use the tm_dots() function. Additionally, as there are 28,000 event points located on a relatively small geographical area, we can expect the points to be dense enough such that we are likely to see clusters without being able to visually make out their densities or individual points.\nTo help with this problem, we can visualise the data using an interactive map style so that we may zoom into specific areas and reduce the density of our visualisation. Normally this can be achieved by specifying tmap_mode() to “view” but as such visualisations are large and computationally expensive, it is in our interest to build the in R and embed them as a HTML widget thus, avoiding building them during the render process. To do so, we leave it in “plot” mode in order to transform them to a leaflet object using the tmap_leaflet() function. The resulting leaflet object can then be saved as a HTML widget using the saveWidget() function from the htmlwidgets package.\n\n\nReveal Code\ngrab_origins_interactive &lt;- tm_shape(grab_origin_sf) +\n  tm_dots(alpha = 0.4, size = 0.03, col = \"green\") +\n  tm_layout(main.title = \"Grab-Posisi Origin points across Singapore\")\n\ngrab_origins_interactive = tmap_leaflet(grab_origins_interactive)\n\nsaveWidget(widget = grab_origins_interactive, file = \"Screenshots/grab_origins_interactive.html\")\n\n\n\n\n\n\nIf we zoom in, it appears that there are little to no overlaps of point coordinates in the data. This is to be expected for 2 reasons. First, pick-up locations are unlikely to fall in the exact same location especially when collected for 2 weeks only. Secondly, the data is very precise (to the cm) when considering that it exists on a country-wide scale. We know this as the data is precise to 2 decimal places when converted to SVY21 and SVY21 geographical points are listed in meters.\nTo be sure, we can use the following code to check for duplicated points.\n\n\nReveal Code\nany(duplicated(grab_origin_ppp))\n\n\n[1] FALSE\n\n\nThis is a good finding as it confirms that there is no need to account for point duplication in the dataset.\nNow, we need to solve the next problem which is the scope of our analysis. To do so, we can look for general point clusters across Singapore which might be of interest to us. A quick visualisation using the coastal outline previously generated as the base layer is performed to help with this.\n\n\nReveal Code\ngrab_origin_sg_plot &lt;- tm_shape(coastal_outline) +\n  tm_polygons(col = \"lightyellow\", border.col = \"blue\", lwd = 1.5) +\n  tm_layout(main.title = \"Origins of Grab Trips in Singapore (within Mainland only)\") +\n  tm_shape(grab_origin_sf) +\n  tm_dots(col = \"darkgreen\", alpha = 0.5)\n\ntmap_save(grab_origin_sg_plot, \"Screenshots/Grab_Origins_SG.png\")\n\n\n\nFrom the darker colouration, it appears that Grab calls in our data were much more concentrated in the central-southern part of Singapore than pretty much any other region. This of course makes sense as that region is where the central business district (CBD) lies, and considering the difference in cost of Grab vs. other forms of public transport, it should be expected that Grab customers are more likely fall within the CBD area especially towards the end of the workday.\nConsidering the size of the Grab origins data and the complexity of the road network, it makes sense to try to limit the area of study so our machines can handle the computations in reasonable time. Considering the spread of the data, the central region is chosen as our area of focus.\n\n\nLimiting the scope of the study\n\nExtracting the CBD polygons\nUsing the Urban Redevelopment Authority of Singapore’s (URA) definition of the CBD, the planning areas which makes it up are extracted from the mpsz spatial data. These polygons are identified using the “PLN_AREA_N” feature in the data, and extracted using base dataframe splicing methods and the %in% operator in R.\n\n\nReveal Code\ncentral_sg &lt;- mpsz2019[mpsz2019$PLN_AREA_N %in% c(\"DOWNTOWN CORE\", \"MARINA EAST\", \"MARINA SOUTH\", \"MUSEUM\", \"NEWTON\", \"ORCHARD\", \"OUTRAM\", \"RIVER VALLEY\", \"ROCHOR\", \"SINGAPORE RIVER\", \"STRAITS VIEW\"), ]\n\nsummary(central_sg)\n\n\n  SUBZONE_N          SUBZONE_C          PLN_AREA_N         PLN_AREA_C       \n Length:50          Length:50          Length:50          Length:50         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n   REGION_N           REGION_C                  geometry \n Length:50          Length:50          MULTIPOLYGON :50  \n Class :character   Class :character   epsg:3414    : 0  \n Mode  :character   Mode  :character   +proj=tmer...: 0  \n\n\nThe spliced sf object is assigned to central_sg and a summary of the data reveals that it contains 50 spatial polygon objects. Much smaller than the 332 polygon features that were originally in the mpsz dataset.\n\n\nVisualising the geograpical scope of study\nTo visualise the new geographical area of study:\n\n\nReveal Code\ncentral_region_plot &lt;- tm_shape(mpsz2019) +\n  tm_polygons(col = \"lightyellow\", border.col = \"navy\") +\n  tm_layout(main.title = \"Central Region of Singapore\") +\n  tm_shape(central_sg) +\n  tm_polygons(col = \"orange\", border.col = \"red\")\n\ntmap_save(central_region_plot, \"Screenshots/central_region_SG.png\")\n\n\n\nFrom the visualisation, we can confirm that the spliced polygons comprises of roughly the area of interest which we identified earlier and that we have reduced the boundaries of our data considerably (much more than the 332 → 50 polygon reduction would suggest).\nAlso, we can confirm that none of Singapore’s outer islands are involved in the focus area we have chosen, and as such we would not need to further narrow down the area to rid of any such polygons.\nAs the focus of the study is the CBD as a whole, and not specific planning areas within the CBD, it makes sense to recomputed the sliced data to consist of the outer boundaries of the CBD only. Like before, the st_union() function is used.\n\n\nReveal Code\ncentral_sg &lt;- st_union(central_sg)\n\n\nA visualisation of the computed boundary polygon can again be visualised by layering it upon the mpsz spatial data using multiple calls to the tm_polygons() function.\n\n\nReveal Code\ncentral_region_SG_outline &lt;- tm_shape(mpsz2019) +\n  tm_polygons(col = \"lightyellow\", border.col = \"navy\") +\n  tm_layout(main.title = \"Central Region of Singapore\") +\n  tm_shape(central_sg) +\n  tm_polygons(col = \"orange\", border.col = \"red\", lwd = 2)\n\ntmap_save(central_region_SG_outline, \"Screenshots/central_region_SG_outline.png\")\n\n\n\n\n\n\nFiltering down the Grab call origins data to match the study area\nNow that we have dictated the study area for the rest of the study, we will also need to trim down the Grab call origins data that we have. This can be achieve via the st_intersection() function to reduce the data to only those within the boundaries of the central_sg polygon we produced.\nA further interesting point of study might be the difference in calls between the weekends and weekdays. As mentioned in the justification for choosing the CBD as the scope, we expect that most of the Grab call events occur as a result of office workers travelling home. Thus, as most offices are closed on the weekends, it will be interesting to examine the difference in call patterns during the work week vs. the weekends . To do so, we will break the data into 2 parts using the weekday column generated prior to identify calls that fall on the weekends.\n\n\nReveal Code\ngrab_weekend &lt;- grab_origin_sf[grab_origin_sf$weekday %in% c(\"Sun\", \"Sat\"), ]\ngrab_weekday &lt;- grab_origin_sf[!grab_origin_sf$weekday %in% c(\"Sun\", \"Sat\"), ]\n\ncentral_grab_weekend_sf &lt;- st_intersection(grab_weekend, central_sg)\ncentral_grab_weekday_sf &lt;- st_intersection(grab_weekday, central_sg)\n\nsummary(central_grab_weekend_sf)\n\n\n    trj_id          driving_mode          osname         \n Length:698         Length:698         Length:698        \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n pingtimestamp                        speed           bearing     \n Min.   :2019-04-13 00:31:23.00   Min.   :-1.000   Min.   :  0.0  \n 1st Qu.:2019-04-13 16:50:57.00   1st Qu.: 3.417   1st Qu.: 53.0  \n Median :2019-04-14 14:02:41.00   Median : 8.527   Median :172.5  \n Mean   :2019-04-17 02:11:19.36   Mean   : 8.497   Mean   :172.9  \n 3rd Qu.:2019-04-20 15:51:27.25   3rd Qu.:13.170   3rd Qu.:299.0  \n Max.   :2019-04-21 23:25:40.00   Max.   :24.620   Max.   :358.0  \n                                                                  \n    accuracy       weekday            geometry  \n Min.   :  2.000   Sun:308   POINT        :698  \n 1st Qu.:  4.354   Mon:  0   epsg:3414    :  0  \n Median :  8.788   Tue:  0   +proj=tmer...:  0  \n Mean   : 10.932   Wed:  0                      \n 3rd Qu.: 13.490   Thu:  0                      \n Max.   :100.000   Fri:  0                      \n                   Sat:390                      \n\n\nReveal Code\nsummary(central_grab_weekday_sf)\n\n\n    trj_id          driving_mode          osname         \n Length:2335        Length:2335        Length:2335       \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n pingtimestamp                        speed           bearing     \n Min.   :2019-04-08 00:09:48.00   Min.   :-1.000   Min.   :  0.0  \n 1st Qu.:2019-04-10 09:36:16.50   1st Qu.: 2.631   1st Qu.: 48.5  \n Median :2019-04-12 22:38:39.00   Median : 7.583   Median :152.0  \n Mean   :2019-04-13 23:06:58.46   Mean   : 7.821   Mean   :166.7  \n 3rd Qu.:2019-04-17 12:29:12.50   3rd Qu.:12.196   3rd Qu.:293.0  \n Max.   :2019-04-19 23:12:41.00   Max.   :26.160   Max.   :359.0  \n                                                                  \n    accuracy       weekday            geometry   \n Min.   :  2.000   Sun:  0   POINT        :2335  \n 1st Qu.:  4.141   Mon:441   epsg:3414    :   0  \n Median :  8.576   Tue:454   +proj=tmer...:   0  \n Mean   : 11.581   Wed:457                       \n 3rd Qu.: 13.000   Thu:540                       \n Max.   :728.000   Fri:443                       \n                   Sat:  0                       \n\n\nA quick examination of the data shows that there were 2335 trips which originated on weekdays, and 698 calls on the weekends. In general, we can also expect around 25 less calls on Saturdays than during the work week and roughly 60 less calls on Sundays. Thursdays also generated the most calls at 540 trip originations happening on thursdays across the 2 weeks.\n\nVisualising the origins of Grab trips over the weekends and weekdays in the Central Region of Singapore.\nTo visualise the rough density patterns across the 2 different time periods in the CBD, we can employ the following code:\n\n\nReveal Code\ngrab_central_weekday_plot &lt;- tm_shape(central_sg) +\n  tm_polygons(col = \"lightyellow\", border.col = \"blue\", lwd = 1.5) +\n  tm_layout(main.title = \"Origins of Weekday Grab Trips in Singapore (within Central Region only)\", main.title.size = 0.7) +\n  tm_shape(central_grab_weekday_sf) +\n  tm_dots(col = \"darkgreen\", alpha = 0.3, size = 0.5)\n\ngrab_central_weekend_plot &lt;- tm_shape(central_sg) +\n  tm_polygons(col = \"lightyellow\", border.col = \"blue\", lwd = 1.5) +\n  tm_layout(main.title = \"Origins of Weekend Grab Trips in Singapore (within Central Region only)\", main.title.size = 0.7) +\n  tm_shape(central_grab_weekend_sf) +\n  tm_dots(col = \"darkgreen\", alpha = 0.3, size = 0.5)\n\ntmap_save(grab_central_weekday_plot, \"Screenshots/grab_central_weekday_plot.png\")\n\ntmap_save(grab_central_weekend_plot, \"Screenshots/grab_central_weekend_plot.png\")\n\n\n\n\ntm_dots() is used to identify the Grab trip origins and a reasonably low alpha value is specified to introduce transparency in the visualisation so that areas of denser events will stand out with deeper colours.\nIt appears that on the weekends, calls not only originate from the CBD less often. Although this is neither obvious nor interpretable from the visualisation alone as we are comparing 10 days of data vs. 4 days worth. However, our quick investigation of the spliced data above does make this a reasonable observation to have here.\nAdditionally, it seems also that the geographical patterns in Grab calls also differ on the weekends and weekdays. On the weekdays the Grab trip origins appear to be more evenly distributed whilst being slightly more concentrated in the nothern regions of the CBD on the weekends with barely any calls from the center and southern parts during this period.\n\n\nTransforming the filtered Grab origins data into Spatial Points, generic sp and ppp objects\nAs before with the simple features Grab data we need to transform the data to a ppp object if we want to conduct analysis on it using the spatstats package. To do so:\n\nFor the Weekend:\n\n\nReveal Code\ncentral_grab_weekend_spatial &lt;- central_grab_weekend_sf %&gt;%\n  as_Spatial()\n\nsummary(central_grab_weekend_spatial)\n\n\nObject of class SpatialPointsDataFrame\nCoordinates:\n               min      max\ncoords.x1 26935.13 31786.09\ncoords.x2 27890.04 32876.01\nIs projected: TRUE \nproj4string :\n[+proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1\n+x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0\n+units=m +no_defs]\nNumber of points: 698\nData attributes:\n    trj_id          driving_mode          osname         \n Length:698         Length:698         Length:698        \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n pingtimestamp                        speed           bearing     \n Min.   :2019-04-13 00:31:23.00   Min.   :-1.000   Min.   :  0.0  \n 1st Qu.:2019-04-13 16:50:57.00   1st Qu.: 3.417   1st Qu.: 53.0  \n Median :2019-04-14 14:02:41.00   Median : 8.527   Median :172.5  \n Mean   :2019-04-17 02:11:19.36   Mean   : 8.497   Mean   :172.9  \n 3rd Qu.:2019-04-20 15:51:27.25   3rd Qu.:13.170   3rd Qu.:299.0  \n Max.   :2019-04-21 23:25:40.00   Max.   :24.620   Max.   :358.0  \n                                                                  \n    accuracy       weekday  \n Min.   :  2.000   Sun:308  \n 1st Qu.:  4.354   Mon:  0  \n Median :  8.788   Tue:  0  \n Mean   : 10.932   Wed:  0  \n 3rd Qu.: 13.490   Thu:  0  \n Max.   :100.000   Fri:  0  \n                   Sat:390  \n\n\nReveal Code\ncentral_grab_weekend_sp &lt;- central_grab_weekend_spatial %&gt;%\n  as(\"SpatialPoints\")\n\nsummary(central_grab_weekend_sp)\n\n\nObject of class SpatialPoints\nCoordinates:\n               min      max\ncoords.x1 26935.13 31786.09\ncoords.x2 27890.04 32876.01\nIs projected: TRUE \nproj4string :\n[+proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1\n+x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0\n+units=m +no_defs]\nNumber of points: 698\n\n\nReveal Code\ncentral_grab_weekend_ppp &lt;- central_grab_weekend_sp %&gt;%\n  as(\"ppp\")\n\nsummary(central_grab_weekend_ppp)\n\n\nPlanar point pattern:  698 points\nAverage intensity 2.885876e-05 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: rectangle = [26935.13, 31786.09] x [27890.04, 32876.01] units\n                    (4851 x 4986 units)\nWindow area = 24186800 square units\n\n\n\n\nFor the Weekdays:\n\n\nReveal Code\ncentral_grab_weekday_spatial &lt;- central_grab_weekday_sf %&gt;%\n  as_Spatial()\n\nsummary(central_grab_weekday_spatial)\n\n\nObject of class SpatialPointsDataFrame\nCoordinates:\n               min      max\ncoords.x1 26902.48 32829.73\ncoords.x2 28050.48 33252.24\nIs projected: TRUE \nproj4string :\n[+proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1\n+x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0\n+units=m +no_defs]\nNumber of points: 2335\nData attributes:\n    trj_id          driving_mode          osname         \n Length:2335        Length:2335        Length:2335       \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n pingtimestamp                        speed           bearing     \n Min.   :2019-04-08 00:09:48.00   Min.   :-1.000   Min.   :  0.0  \n 1st Qu.:2019-04-10 09:36:16.50   1st Qu.: 2.631   1st Qu.: 48.5  \n Median :2019-04-12 22:38:39.00   Median : 7.583   Median :152.0  \n Mean   :2019-04-13 23:06:58.46   Mean   : 7.821   Mean   :166.7  \n 3rd Qu.:2019-04-17 12:29:12.50   3rd Qu.:12.196   3rd Qu.:293.0  \n Max.   :2019-04-19 23:12:41.00   Max.   :26.160   Max.   :359.0  \n                                                                  \n    accuracy       weekday  \n Min.   :  2.000   Sun:  0  \n 1st Qu.:  4.141   Mon:441  \n Median :  8.576   Tue:454  \n Mean   : 11.581   Wed:457  \n 3rd Qu.: 13.000   Thu:540  \n Max.   :728.000   Fri:443  \n                   Sat:  0  \n\n\nReveal Code\ncentral_grab_weekday_sp &lt;- central_grab_weekday_spatial %&gt;%\n  as(\"SpatialPoints\")\n\nsummary(central_grab_weekday_sp)\n\n\nObject of class SpatialPoints\nCoordinates:\n               min      max\ncoords.x1 26902.48 32829.73\ncoords.x2 28050.48 33252.24\nIs projected: TRUE \nproj4string :\n[+proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1\n+x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0\n+units=m +no_defs]\nNumber of points: 2335\n\n\nReveal Code\ncentral_grab_weekday_ppp &lt;- central_grab_weekday_sp %&gt;%\n  as(\"ppp\")\n\nsummary(central_grab_weekday_ppp)\n\n\nPlanar point pattern:  2335 points\nAverage intensity 7.573262e-05 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: rectangle = [26902.48, 32829.73] x [28050.48, 33252.24] units\n                    (5927 x 5202 units)\nWindow area = 30832200 square units\n\n\nAgain, summaries of each casting step are displayed to our purview.\n\n\n\nScaling the ppp objects to “km” for more interpretable patterns when we produce the kernel density estimates\nAs mentioned prior, the data is scaled in meters. This means that when we generate the kernel density estimates (KDE) we will be given densities which suggests that certain areas receive 0.0005 calls per meter2 for instance as we are running a KDE across an entire country. This has very low interpretability, so we should re-scale the data points to km so that we can receive easy to understand density estimates when we conduct KDE.\nTo do so, we rely upon the rescale() function, parsing the conversion rate of 1000 to the function:\n\n\nReveal Code\ncentral_grab_weekend_ppp.km &lt;- rescale(central_grab_weekend_ppp, 1000, \"km\")\n\ncentral_grab_weekday_ppp.km &lt;- rescale(central_grab_weekday_ppp, 1000, \"km\")\n\nsummary(central_grab_weekend_ppp.km)\n\n\nPlanar point pattern:  698 points\nAverage intensity 28.85876 points per square km\n\nCoordinates are given to 6 decimal places\n\nWindow: rectangle = [26.93513, 31.78609] x [27.89004, 32.87601] km\n                    (4.851 x 4.986 km)\nWindow area = 24.1868 square km\nUnit of length: 1 km\n\n\nReveal Code\nsummary(central_grab_weekday_ppp.km)\n\n\nPlanar point pattern:  2335 points\nAverage intensity 75.73262 points per square km\n\nCoordinates are given to 6 decimal places\n\nWindow: rectangle = [26.90248, 32.82973] x [28.05048, 33.25224] km\n                    (5.927 x 5.202 km)\nWindow area = 30.8322 square km\nUnit of length: 1 km\n\n\nSummaries of the Grab trip origins planar point pattern objects now show that the points have successfully been re-scaled to km."
  },
  {
    "objectID": "Take-Home_Exercise/Take-Home_Exercise_01/Take-Home_Exercise_01.html#conducting-and-analysing-kernel-density-estimation",
    "href": "Take-Home_Exercise/Take-Home_Exercise_01/Take-Home_Exercise_01.html#conducting-and-analysing-kernel-density-estimation",
    "title": "Take-Home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab’s hailing services in Singapore",
    "section": "Conducting and Analysing Kernel Density Estimation",
    "text": "Conducting and Analysing Kernel Density Estimation\n\nUsing the ppl method to conduct the Kernel Density Estimations\nThe ppl method is favoured here because the pre-emptive visualisation showed us that the call origins are generally spread out in multiple clusters over the week. As such, we do not use the diggle bandwidth method as we are not searching for a single cluster within a bunch of noise but rather multiple clusters.\nThe KDE are computed using the density() function from spatstat, specifying bw.ppl for the kernel bandwidth. Especially as there are large clusters near the edge of our project’s geographical boundaries observable from earlier visualisations, it is imperative that we correct for edge bias by specifying it to TRUE.\n\n\nReveal Code\npar(cex.main = 0.7)\n\nkde_central_grab_weekend_ppl_bw.km &lt;- density(central_grab_weekend_ppp.km, sigma=bw.ppl, edge=TRUE, kernel=\"gaussian\")\n\nkde_central_grab_weekday_ppl_bw.km &lt;- density(central_grab_weekday_ppp.km, sigma=bw.ppl, edge=TRUE, kernel=\"gaussian\")\n\nplot(kde_central_grab_weekend_ppl_bw.km, main = \"Scaled Kernel Density Estimate of Grab Trip Origins in Central SG on Weekends (ppl)\")\n\n\n\n\n\n\n\n\n\nReveal Code\nplot(kde_central_grab_weekday_ppl_bw.km, main = \"Scaled Kernel Density Estimate of Grab Trip Origins in Central SG on Weekdays (ppl)\")\n\n\n\n\n\n\n\n\n\nFrom the produced KDE plots, we can confirm that the Grab trip origins are pretty evenly spread out in multiple clusters across the CBD on the weekdays. However, the trips are far more concentrated in the northern half of the CBD than we initially realised, with 2 big hotspots in there which stand out from the rest.\n\n\nUsing the Adaptive Bandwidth method to conduct the Kernel Density Estimations\nAn adaptive bandwidth method can also be applied to our kernel density estimates instead:\n\n\nReveal Code\npar(cex.main = 0.7)\n\nkde_central_grab_weekend_ppp_adaptive &lt;- adaptive.density(central_grab_weekend_ppp.km, method=\"kernel\")\n\nkde_central_grab_weekday_ppp_adaptive &lt;- adaptive.density(central_grab_weekday_ppp.km, method=\"kernel\")\n\nplot(kde_central_grab_weekend_ppp_adaptive, main = \"Kernel Density Estimate of Grab Trip Origins in central SG on weekends (adaptive bandwidth)\")\n\n\n\n\n\n\n\n\n\nReveal Code\nplot(kde_central_grab_weekday_ppp_adaptive, main = \"Kernel Density Estimate of Grab Trip Origins in central SG on weekdays (adaptive bandwidth)\")\n\n\n\n\n\n\n\n\n\nThe adaptive KDE plots tells a similar story to when the ppl bandwidth method was applied. However, this time, the clustering of the origins of Grab trips towards the northern half of the CBD during the weekends are far more evident than before. In fact, when adapative bandwidth is applied, the southern half of the CBD is what can only be described as a cold-spot with barely any purple-ish colouration present. The evidence of multiple but well-spread out clusters across the CBD during the weekdays is more evident here than before as previously it seems like the hot-spots may have been briefly connected, here, the drop-off between each hot-spot is much more obvious.\n\n\nConducting the Clark-Evans test for Spatial Point Pattern\nTo conduct a Clark-Evans’ hypothesis test as to the randomness of the spatial point distribution of Grab trips in the CBD, we can utilise the clarkevans.test() function. Within the function, we can correct for edge bias again, especially now that we know from the KDE that we can expect the data to be heavily concentrated near the top during the weekends. CDF correction method is applied as recommended in the Clark-Evans documentation in R which suggests Donnelly correction for rectangular windows only.\n\n\nReveal Code\nclarkevans.test(central_grab_weekday_ppp.km, correction=\"cdf\", alternative=c(\"clustered\"), nsim=99)\n\n\n\n    Clark-Evans test\n    CDF correction\n    Z-test\n\ndata:  central_grab_weekday_ppp.km\nR = 0.30295, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\nReveal Code\nclarkevans.test(central_grab_weekend_ppp.km, correction=\"cdf\", alternative=c(\"clustered\"), nsim=99)\n\n\n\n    Clark-Evans test\n    CDF correction\n    Z-test\n\ndata:  central_grab_weekend_ppp.km\nR = 0.41096, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\nFrom the p-values of the Clark-Evan’s test, we have to conclude that the distributions of the Grab trip origins display clustering patterns within Singapore’s CBD regardless of the part of the week. This is a conclusion supported by our KDE plots either using ppl or adaptive bandwidth methods."
  },
  {
    "objectID": "Take-Home_Exercise/Take-Home_Exercise_01/Take-Home_Exercise_01.html#conducting-and-analysing-network-constrained-kernel-density-estimations",
    "href": "Take-Home_Exercise/Take-Home_Exercise_01/Take-Home_Exercise_01.html#conducting-and-analysing-network-constrained-kernel-density-estimations",
    "title": "Take-Home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab’s hailing services in Singapore",
    "section": "Conducting and analysing Network-Constrained Kernel Density Estimations",
    "text": "Conducting and analysing Network-Constrained Kernel Density Estimations\n\nLimiting the Road Network Data to Central Singapore\nEarlier in this report, we conducted an extraction of the road networks from OpenStreetMap using the st_intersection() function. We will need to do so again as we have since further limited the scope of our study to the CBD. This time however, as the purpose of using the road network data lies in constructing Network-Contrained Kernel Density Estimations, we will need to ensure that only spatial Line features exist in the data. To do so, we can employ the st_cast() function to coerce any other spatial features to Spatial Lines.\n\n\nReveal Code\ncentral_roads &lt;- st_intersection(roads, central_sg)\n\ncentral_roads &lt;- st_cast(central_roads, \"LINESTRING\")\n\nwrite_rds(central_roads, \"Data/Geospatial/RDS/central_road_network.rds\")\n\n\nOnce again we save the spliced data for further use as it is computationally expensive.\n\n\nReveal Code\ncentral_roads &lt;- read_rds(\"Data/Geospatial/RDS/central_road_network.rds\")\n\ncentral_roads &lt;- st_simplify(central_roads, preserveTopology = FALSE)\n\n\n\nVisualising the Road Network Data across the CBD\nUsing the tm_lines() function we can build a visual plot of the road networks in the CBD\n\n\nReveal Code\ncentral_road_network_plot &lt;- tm_shape(central_sg) +\n  tm_polygons(col = \"lightyellow\") +\n  tm_layout(main.title = \"Road Network in Central Singapore\") +\n  tm_shape(central_roads) +\n  tm_lines(col = \"brown\")\n\ntmap_save(central_road_network_plot, \"Screenshots/central_road_network.png\")\n\n\n\n\n\n\nPreparing the lixel objects and generating the line centre points\nIn order to conduct NKDE, we will need to use the spNetwork package which requires that we parse lixel objects rather than sf objects with LINESTRING geometry type. Thus, we need to generate these lixels using the lixelize_lines() function. Additionally, the computation of NKDE also requires that we generate the center points of these lixels. This can be generated by parsing the lixels into the lines_center() function.\n\n\nReveal Code\nlixels &lt;- lixelize_lines(central_roads, 700, mindist = 300)\n\nsamples &lt;- lines_center(lixels)\n\nwrite_rds(lixels, \"Data/Geospatial/RDS/central_road_lixels.rds\")\n\nwrite_rds(samples, \"Data/Geospatial/RDS/central_road_centrepoints.rds\")\n\n\nAs the road network data is quite extensive, the computation of the lixels are extremely computationally expensive and time consuming. Thus, we save the generated lixels and lixel center points as RDS files also.\n\n\nReveal Code\nlixels &lt;- read_rds(\"Data/Geospatial/RDS/central_road_lixels.rds\")\n\nsamples &lt;- read_rds(\"Data/Geospatial/RDS/central_road_centrepoints.rds\")\n\n\n\n\nPerforming the Network Constrained Kernel Density Estimation\nNow that we have prepared the lixels and their center points, we can run the NKDE across our 2 segmented Grab data. Our previous KDE plots provides us with some indicates of the parameters in which we can run our NKDE upon. Firstly, the kernel used should be “quartic” as we are looking for several spikes in the data whilst being less sensitive to outlier events because we know that the southern parts of the CBD are less densely serviced by Grab than the northern parts during the weekends. And even though the data is more evenly spread out in the workweek, the adaptive KDE reveals that the data exists as multiple hot-spots with strong drop-offs rather than a smooth transition to the next hot-spot. We run it on a “simple” method and set sparse to TRUE to reduce computation time, but also set verbose to TRUE to get updated that the process is running properly as the computation is expected to be very time-consuming regardless.\n\nFor the Weekday Data:\nUsing the nkde() function to compute the network-constrained kernel density estimates of the weekday Grab trips in the CBD:\n\n\nReveal Code\nweekday_densities &lt;- nkde(central_roads, \n                          events = central_grab_weekday_sf,  \n                          w = rep(1,nrow(central_grab_weekday_sf)), \n                          samples = samples, \n                          kernel_name = \"quartic\", \n                          bw = 300, \n                          div= \"bw\", \n                          method = \"simple\", \n                          digits = 1, \n                          tol = 1, \n                          grid_shape = c(1,1), \n                          max_depth = 8, \n                          agg = 5, \n                          sparse = TRUE, \n                          verbose = TRUE)\n\nwrite_rds(weekday_densities, \"Data/Geospatial/RDS/Grab_weekday_nkde.rds\")\n\n\nOnce the density estimates are computed, we store them away as RDS files to avoid running them again unless there is a change in the data.\n\n\nReveal Code\nweekday_densities &lt;- read_rds(\"Data/Geospatial/RDS/Grab_weekday_nkde.rds\")\n\nsamples$density &lt;- weekday_densities * 1000\nlixels$density &lt;- weekday_densities * 1000\n\n\nWe then reassign the computed densities to the lixel and lixel center point objects used.\n\n\nVisualising the Network Constrained Kernel Density Estimate of the Weekday Grab Data over an interactive OpenStreetMap layer\nUsing the densities appended to the lixels generated, and the tm_lines() and tm_dots() function, we can visualise the NKDE plots. Furthermore, we shall use tm_basemap(“OpenStreetMap”) as the base layer rather than the central_sg polygons previously used. This will allow us to obtain contextualised information of the hot-spots from the NKDE produced.\n\n\nReveal Code\ngrab_weekday_interactive &lt;-  tm_shape(lixels) +\n  tm_lines(col=\"density\", lwd = 2) +\n  tm_layout(frame = TRUE, main.title = \"Network-Constrained Kernel Density Estimation of Grab Call Origins in the CBD\") +\n  tm_shape(central_grab_weekday_sf) +\n  tm_dots(col = \"green\", alpha = 0.5, size = 0.01) +\n  tm_scale_bar() +\n  tm_basemap(\"OpenStreetMap\")\n\ngrab_weekday_interactive &lt;- tmap_leaflet(grab_weekday_interactive)\n\nsaveWidget(grab_weekday_interactive, \"Screenshots/CBD_grab_weekday_interactive.html\")\n\n\n\n\n\n\nExamining the network constrained kernel density estimation plot for Grab trips in the CBD on weekdays, we can see that there are 5 main clusters which have been formed in the following areas:\n\nIn Orchard, around the junctions near Orchard Ion Shopping Center.\nAlong Somerset and Orchard Roads, between Somerset and Dhoby Ghaut MRT stations.\nAround Rochor and Bugis.\nChinatown\nRaffles Place\n\nIntuitively, we this makes sense as 1. and 2. are huge shopping districts in addition to housing plenty of offices. As such, we should expect them to be frequented by tourist who may be more inclined to using Grab, as well as office workers returning home from the area after work or a night out. For hot spot 3., even though less offices are expected there, there are more nightlife spots in the area which could explain why a cluster if formed there as well.\nThe hot-spots 4. and 5. are especially strong when compared to the other 3. This is likely a result of the concentration of financial institutions and large multinational corporations concentrated in the area unlike around the other 3 clusters. Thus, an explanation for hot spots 4 and 5 would be that demand from office workers returning home either from work or a night out is strongly contributing to Grab demand in these parts of the CBD.\n\n\nFor the Weekend Data:\nUsing the nkde() function to compute the network-constrained kernel density estimates of the weekend Grab trips in the CBD:\n\n\nReveal Code\nweekend_densities &lt;- nkde(central_roads,\n                  events = central_grab_weekend_sf,\n                  w = rep(1,nrow(central_grab_weekend_sf)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300,\n                  div= \"bw\",\n                  method = \"simple\",\n                  digits = 1,\n                  tol = 1,\n                  grid_shape = c(1,1),\n                  max_depth = 8,\n                  agg = 5,\n                  sparse = TRUE,\n                  verbose = TRUE)\n\nwrite_rds(weekend_densities, \"Data/Geospatial/RDS/Grab_weekend_nkde.rds\")\n\n\n\n\nReveal Code\nweekend_densities &lt;- read_rds(\"Data/Geospatial/RDS/Grab_weekend_nkde.rds\")\n\nsamples$density &lt;- weekend_densities * 1000\nlixels$density &lt;- weekend_densities * 1000\n\n\n\n\nVisualising the Network Constrained Kernel Density Estimate of the Weekend Grab Data over an interactive OpenStreetMap layer\nSimilarly, like in the visualisation of the weekday Grab data, we can visualise the weekend Grab data within the CBD on weekends:\n\n\nReveal Code\ngrab_weekend_interactive &lt;-  tm_shape(lixels) +\n  tm_lines(col=\"density\", lwd = 2) +\n  tm_layout(frame = TRUE) +\n  tm_shape(central_grab_weekend_sf) +\n  tm_dots(col = \"green\", alpha = 0.5, size = 0.01) +\n  tm_scale_bar() +\n  tm_basemap(\"OpenStreetMap\")\n\ngrab_weekend_interactive &lt;- tmap_leaflet(grab_weekend_interactive)\n\nsaveWidget(grab_weekend_interactive, \"Screenshots/CBD_grab_weekend_interactive.html\")\n\n\n\n\n\n\nLooking at the network constrained kernel density estimation plot generated for Grab trips in the CBD on weekends, we cannot pin point any new and significant clusters when compared to the NKDE generated for the weekdays. There are 2 main differences however. The region around Fort Canning Park and the Esplanade appear to be slightly more dense than before, and clusters 4. and 5. have all but disappeared. It is possible that people are leaving from Fort Canning and the Esplanade more often on the weekends, but a more like cause is that the disappearance of clusters 4. and 5. and biased the NKDE computation slightly in these areas.\nThe visualisation of the weekend NKDE gives credence to the earlier suggestion that office workers are the heaviest contributors to the Grab call demand in Chinatown and Raffles Place as majority of offices are not opened on the weekends which would explain the tank in the Grab call demand."
  },
  {
    "objectID": "Hands-On_Exercise/Hands-On_Exercise_01/Hands-On_Exercise_01.html#section",
    "href": "Hands-On_Exercise/Hands-On_Exercise_01/Hands-On_Exercise_01.html#section",
    "title": "Hands-On Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.3",
    "text": "1.3\n\n\nReveal Code\n#Installing the pacman package\n#install.packages('pacman')\npacman::p_load(sf, tidyverse) \n\n\n#1.4\n\n\nReveal Code\n#1.4.1\nmpsz = st_read(dsn = \"Data/Geospatial/Master Plan\", \n               layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\Users\\yungq\\Desktop\\SMU Modules\\Y4S1\\Geospatial Analysis and Applications\\IS415 Course Website\\Hands-On_Exercise\\Hands-On_Exercise_01\\Data\\Geospatial\\Master Plan' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nReveal Code\n#1.4.2\ncyclingpath = st_read(dsn = \"Data/Geospatial/CyclingPath\", \n                      layer = \"CyclingPathGazette\")\n\n\nReading layer `CyclingPathGazette' from data source \n  `C:\\Users\\yungq\\Desktop\\SMU Modules\\Y4S1\\Geospatial Analysis and Applications\\IS415 Course Website\\Hands-On_Exercise\\Hands-On_Exercise_01\\Data\\Geospatial\\CyclingPath' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2558 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\n\nReveal Code\n#1.4.3\npreschool = st_read(\"Data/Geospatial/Pre-School Location/PreSchoolsLocation.kml\")\n\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\Users\\yungq\\Desktop\\SMU Modules\\Y4S1\\Geospatial Analysis and Applications\\IS415 Course Website\\Hands-On_Exercise\\Hands-On_Exercise_01\\Data\\Geospatial\\Pre-School Location\\PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n#1.5\n\n\nReveal Code\n#1.5.1\nst_geometry(mpsz)\n\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nReveal Code\n#1.5.2\nglimpse(mpsz)\n\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\nReveal Code\n#1.5.3\nhead(mpsz, n = 5)\n\n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\n#1.6\n\n\nReveal Code\nplot(mpsz)\n\n\n\n\n\n\n\n\n\nReveal Code\nplot(st_geometry(mpsz))\n\n\n\n\n\n\n\n\n\nReveal Code\nplot(mpsz[\"PLN_AREA_N\"])\n\n\n\n\n\n\n\n\n\n#1.7\n\n\nReveal Code\n#1.7.1\nst_crs(mpsz)\n\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nReveal Code\nmpsz3414 &lt;- st_set_crs(mpsz, 3414)\nst_crs(mpsz3414)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nReveal Code\n#1.7.2\npreschool3414 &lt;- st_transform(preschool, \n                              crs = 3414)\n\n\n#1.8\n\n\nReveal Code\n#1.8.1\nlistings &lt;- read_csv(\"Data/Aspatial/Airbnb Listing/listings.csv\")\nlist(listings)\n\n\n[[1]]\n# A tibble: 3,457 × 75\n       id listing_url            scrape_id last_scraped source name  description\n    &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt; &lt;date&gt;       &lt;chr&gt;  &lt;chr&gt; &lt;lgl&gt;      \n 1  71609 https://www.airbnb.co…   2.02e13 2023-12-27   previ… Vill… NA         \n 2  71896 https://www.airbnb.co…   2.02e13 2023-12-26   city … Home… NA         \n 3  71903 https://www.airbnb.co…   2.02e13 2023-12-26   city … Home… NA         \n 4 275343 https://www.airbnb.co…   2.02e13 2023-12-26   city … Rent… NA         \n 5 275344 https://www.airbnb.co…   2.02e13 2023-12-26   city … Rent… NA         \n 6 289234 https://www.airbnb.co…   2.02e13 2023-12-27   previ… Home… NA         \n 7 294281 https://www.airbnb.co…   2.02e13 2023-12-27   city … Rent… NA         \n 8 324945 https://www.airbnb.co…   2.02e13 2023-12-26   city … Rent… NA         \n 9 330095 https://www.airbnb.co…   2.02e13 2023-12-26   city … Rent… NA         \n10 369141 https://www.airbnb.co…   2.02e13 2023-12-27   city … Plac… NA         \n# ℹ 3,447 more rows\n# ℹ 68 more variables: neighborhood_overview &lt;chr&gt;, picture_url &lt;chr&gt;,\n#   host_id &lt;dbl&gt;, host_url &lt;chr&gt;, host_name &lt;chr&gt;, host_since &lt;date&gt;,\n#   host_location &lt;chr&gt;, host_about &lt;chr&gt;, host_response_time &lt;chr&gt;,\n#   host_response_rate &lt;chr&gt;, host_acceptance_rate &lt;chr&gt;,\n#   host_is_superhost &lt;lgl&gt;, host_thumbnail_url &lt;chr&gt;, host_picture_url &lt;chr&gt;,\n#   host_neighbourhood &lt;chr&gt;, host_listings_count &lt;dbl&gt;, …\n\n\nReveal Code\n#1.8.2\nlistings_sf &lt;- st_as_sf(listings, \n                        coords = c(\"longitude\", \"latitude\"), \n                        crs=4326) %&gt;%\n  st_transform(crs = 3414)\n\nglimpse(listings_sf)\n\n\nRows: 3,457\nColumns: 74\n$ id                                           &lt;dbl&gt; 71609, 71896, 71903, 2753…\n$ listing_url                                  &lt;chr&gt; \"https://www.airbnb.com/r…\n$ scrape_id                                    &lt;dbl&gt; 2.023123e+13, 2.023123e+1…\n$ last_scraped                                 &lt;date&gt; 2023-12-27, 2023-12-26, …\n$ source                                       &lt;chr&gt; \"previous scrape\", \"city …\n$ name                                         &lt;chr&gt; \"Villa in Singapore · ★4.…\n$ description                                  &lt;lgl&gt; NA, NA, NA, NA, NA, NA, N…\n$ neighborhood_overview                        &lt;chr&gt; NA, NA, \"Quiet and view o…\n$ picture_url                                  &lt;chr&gt; \"https://a0.muscache.com/…\n$ host_id                                      &lt;dbl&gt; 367042, 367042, 367042, 1…\n$ host_url                                     &lt;chr&gt; \"https://www.airbnb.com/u…\n$ host_name                                    &lt;chr&gt; \"Belinda\", \"Belinda\", \"Be…\n$ host_since                                   &lt;date&gt; 2011-01-29, 2011-01-29, …\n$ host_location                                &lt;chr&gt; \"Singapore\", \"Singapore\",…\n$ host_about                                   &lt;chr&gt; \"Hi My name is Belinda -H…\n$ host_response_time                           &lt;chr&gt; \"N/A\", \"N/A\", \"N/A\", \"wit…\n$ host_response_rate                           &lt;chr&gt; \"N/A\", \"N/A\", \"N/A\", \"100…\n$ host_acceptance_rate                         &lt;chr&gt; \"100%\", \"100%\", \"100%\", \"…\n$ host_is_superhost                            &lt;lgl&gt; FALSE, FALSE, FALSE, FALS…\n$ host_thumbnail_url                           &lt;chr&gt; \"https://a0.muscache.com/…\n$ host_picture_url                             &lt;chr&gt; \"https://a0.muscache.com/…\n$ host_neighbourhood                           &lt;chr&gt; \"Tampines\", \"Tampines\", \"…\n$ host_listings_count                          &lt;dbl&gt; 5, 5, 5, 51, 51, 5, 7, 51…\n$ host_total_listings_count                    &lt;dbl&gt; 15, 15, 15, 68, 68, 15, 8…\n$ host_verifications                           &lt;chr&gt; \"['email', 'phone']\", \"['…\n$ host_has_profile_pic                         &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, T…\n$ host_identity_verified                       &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, T…\n$ neighbourhood                                &lt;chr&gt; NA, NA, \"Singapore, Singa…\n$ neighbourhood_cleansed                       &lt;chr&gt; \"Tampines\", \"Tampines\", \"…\n$ neighbourhood_group_cleansed                 &lt;chr&gt; \"East Region\", \"East Regi…\n$ property_type                                &lt;chr&gt; \"Private room in villa\", …\n$ room_type                                    &lt;chr&gt; \"Private room\", \"Private …\n$ accommodates                                 &lt;dbl&gt; 3, 1, 2, 1, 1, 4, 2, 1, 1…\n$ bathrooms                                    &lt;lgl&gt; NA, NA, NA, NA, NA, NA, N…\n$ bathrooms_text                               &lt;chr&gt; \"1 private bath\", \"Shared…\n$ bedrooms                                     &lt;lgl&gt; NA, NA, NA, NA, NA, NA, N…\n$ beds                                         &lt;dbl&gt; 3, 1, 2, 1, 1, 5, 1, 1, 1…\n$ amenities                                    &lt;chr&gt; \"[]\", \"[]\", \"[]\", \"[]\", \"…\n$ price                                        &lt;chr&gt; \"$150.00\", \"$80.00\", \"$80…\n$ minimum_nights                               &lt;dbl&gt; 92, 92, 92, 60, 60, 92, 9…\n$ maximum_nights                               &lt;dbl&gt; 365, 365, 365, 999, 999, …\n$ minimum_minimum_nights                       &lt;dbl&gt; 92, 92, 92, 60, 60, 92, 9…\n$ maximum_minimum_nights                       &lt;dbl&gt; 92, 92, 92, 60, 60, 92, 9…\n$ minimum_maximum_nights                       &lt;dbl&gt; 1125, 1125, 1125, 1125, 1…\n$ maximum_maximum_nights                       &lt;dbl&gt; 1125, 1125, 1125, 1125, 1…\n$ minimum_nights_avg_ntm                       &lt;dbl&gt; 92, 92, 92, 60, 60, 92, 9…\n$ maximum_nights_avg_ntm                       &lt;dbl&gt; 1125, 1125, 1125, 1125, 1…\n$ calendar_updated                             &lt;lgl&gt; NA, NA, NA, NA, NA, NA, N…\n$ has_availability                             &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, T…\n$ availability_30                              &lt;dbl&gt; 30, 30, 30, 6, 6, 29, 30,…\n$ availability_60                              &lt;dbl&gt; 34, 60, 60, 6, 6, 33, 60,…\n$ availability_90                              &lt;dbl&gt; 55, 90, 90, 6, 6, 54, 90,…\n$ availability_365                             &lt;dbl&gt; 55, 91, 91, 183, 183, 54,…\n$ calendar_last_scraped                        &lt;date&gt; 2023-12-27, 2023-12-26, …\n$ number_of_reviews                            &lt;dbl&gt; 19, 24, 46, 20, 16, 12, 1…\n$ number_of_reviews_ltm                        &lt;dbl&gt; 0, 0, 0, 0, 3, 0, 0, 1, 2…\n$ number_of_reviews_l30d                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ first_review                                 &lt;date&gt; 2011-12-19, 2011-07-30, …\n$ last_review                                  &lt;date&gt; 2020-01-17, 2019-10-13, …\n$ review_scores_rating                         &lt;dbl&gt; 4.44, 4.16, 4.41, 4.40, 4…\n$ review_scores_accuracy                       &lt;dbl&gt; 4.37, 4.22, 4.39, 4.16, 4…\n$ review_scores_cleanliness                    &lt;dbl&gt; 4.00, 4.09, 4.52, 4.26, 4…\n$ review_scores_checkin                        &lt;dbl&gt; 4.63, 4.43, 4.63, 4.47, 4…\n$ review_scores_communication                  &lt;dbl&gt; 4.78, 4.43, 4.64, 4.42, 4…\n$ review_scores_location                       &lt;dbl&gt; 4.26, 4.17, 4.50, 4.53, 4…\n$ review_scores_value                          &lt;dbl&gt; 4.32, 4.04, 4.36, 4.63, 4…\n$ license                                      &lt;chr&gt; NA, NA, NA, \"S0399\", \"S03…\n$ instant_bookable                             &lt;lgl&gt; FALSE, FALSE, FALSE, TRUE…\n$ calculated_host_listings_count               &lt;dbl&gt; 5, 5, 5, 51, 51, 5, 7, 51…\n$ calculated_host_listings_count_entire_homes  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0…\n$ calculated_host_listings_count_private_rooms &lt;dbl&gt; 5, 5, 5, 51, 51, 5, 6, 51…\n$ calculated_host_listings_count_shared_rooms  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ reviews_per_month                            &lt;dbl&gt; 0.13, 0.16, 0.30, 0.15, 0…\n$ geometry                                     &lt;POINT [m]&gt; POINT (41972.5 3639…\n\n\n#1.9\n\n\nReveal Code\n#1.9.1\nbuffer_cycling &lt;- st_buffer(cyclingpath, \n                            dist=5, nQuadSegs = 30)\n\nbuffer_cycling$AREA &lt;- st_area(buffer_cycling)\nsum(buffer_cycling$AREA)\n\n\n1774367 [m^2]\n\n\nReveal Code\n#1.9.2\nmpsz3414$`PreSch Count`&lt;- lengths(st_intersects(mpsz3414, preschool3414))\nsummary(mpsz3414$`PreSch Count`)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\n\nReveal Code\ntop_n(mpsz3414, 1, `PreSch Count`)\n\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           72\n\n\nReveal Code\nmpsz3414$Area &lt;- mpsz3414 %&gt;% \n  st_area()\n\nmpsz3414 &lt;- mpsz3414 %&gt;% \n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)\n\n\n#1.10\n\n\nReveal Code\nhist(mpsz3414$`PreSch Density`)\n\n\n\n\n\n\n\n\n\nReveal Code\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") + \n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\", \n       x = \"Pre-school density (per km sq)\", \n       y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nReveal Code\nggplot(data=mpsz3414, \n       aes(y = `PreSch Count`, \n           x= as.numeric(`PreSch Density`))) + \n  geom_point(color=\"black\", \n             fill=\"light blue\") + \n  xlim(0, 40) + \n  ylim(0, 40) +\n  labs(title = \"\", \n       x = \"Pre-school density (per km sq)\", \n       y = \"Pre-school count\")"
  },
  {
    "objectID": "In-Class_Exercise/In-Class_Exercise_02/In-Class_Exercise_02.html",
    "href": "In-Class_Exercise/In-Class_Exercise_02/In-Class_Exercise_02.html",
    "title": "In-Class Exercise 2: R for Geospatial Data Science",
    "section": "",
    "text": "Loading the following R packages:\n\narrow\nlubridate\ntidyverse\ntmap\nsf\n\n\n\nReveal Code\npacman::p_load(arrow, lubridate, tidyverse, tmap, sf)"
  },
  {
    "objectID": "In-Class_Exercise/In-Class_Exercise_02/In-Class_Exercise_02.html#preparing-the-data-for-use",
    "href": "In-Class_Exercise/In-Class_Exercise_02/In-Class_Exercise_02.html#preparing-the-data-for-use",
    "title": "In-Class Exercise 2: R for Geospatial Data Science",
    "section": "Preparing the data for use",
    "text": "Preparing the data for use\n\n\nReveal Code\ndf$pingtimestamp &lt;- as_datetime(df$pingtimestamp)"
  },
  {
    "objectID": "In-Class_Exercise/In-Class_Exercise_02/In-Class_Exercise_02.html#extracting-the-origin-locations-of-the-trips",
    "href": "In-Class_Exercise/In-Class_Exercise_02/In-Class_Exercise_02.html#extracting-the-origin-locations-of-the-trips",
    "title": "In-Class Exercise 2: R for Geospatial Data Science",
    "section": "Extracting the origin locations of the trips",
    "text": "Extracting the origin locations of the trips\n\n\nReveal Code\norigin_df &lt;- df %&gt;% \n  group_by(trj_id) %&gt;% \n  arrange(pingtimestamp) %&gt;% \n  filter(row_number() == 1) %&gt;% \n  mutate(weekday = wday(pingtimestamp, label = TRUE, abbr = TRUE), start_hr = factor(hour(pingtimestamp)), day = factor(mday(pingtimestamp)))\n\nhead(origin_df)\n\n\n# A tibble: 6 × 12\n# Groups:   trj_id [6]\n  trj_id driving_mode osname  pingtimestamp       rawlat rawlng speed bearing\n  &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt;   &lt;dttm&gt;               &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n1 70895  car          android 2019-04-08 00:09:40   1.38   104.  6.80      41\n2 21926  car          android 2019-04-08 00:09:49   1.29   104. 10.8       68\n3 47498  car          ios     2019-04-08 00:09:52   1.38   104. 18.3      307\n4 41322  car          android 2019-04-08 00:10:00   1.28   104. 18.7      230\n5 18103  car          android 2019-04-08 00:10:09   1.45   104. 14.1      155\n6 64813  car          ios     2019-04-08 00:10:12   1.31   104. 19.8      109\n# ℹ 4 more variables: accuracy &lt;dbl&gt;, weekday &lt;ord&gt;, start_hr &lt;fct&gt;, day &lt;fct&gt;"
  },
  {
    "objectID": "In-Class_Exercise/In-Class_Exercise_02/In-Class_Exercise_02.html#extracting-the-destination-locations-of-the-trips",
    "href": "In-Class_Exercise/In-Class_Exercise_02/In-Class_Exercise_02.html#extracting-the-destination-locations-of-the-trips",
    "title": "In-Class Exercise 2: R for Geospatial Data Science",
    "section": "Extracting the destination locations of the trips",
    "text": "Extracting the destination locations of the trips\n\n\nReveal Code\ndestination_df &lt;- df %&gt;% \n  group_by(trj_id) %&gt;% \n  arrange(desc(pingtimestamp)) %&gt;% \n  filter(row_number() == 1) %&gt;% \n  mutate(weekday = wday(pingtimestamp, label = TRUE, abbr = TRUE), start_hr = factor(hour(pingtimestamp)), day = factor(mday(pingtimestamp)))\n\nhead(destination_df)\n\n\n# A tibble: 6 × 12\n# Groups:   trj_id [6]\n  trj_id driving_mode osname  pingtimestamp       rawlat rawlng speed bearing\n  &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt;   &lt;dttm&gt;               &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n1 54687  car          android 2019-04-21 23:56:37   1.44   104.  1.19     299\n2 81574  car          ios     2019-04-21 23:56:33   1.34   104. 17.4      117\n3 17190  car          android 2019-04-21 23:56:30   1.34   104.  9.09     201\n4 13793  car          android 2019-04-21 23:56:25   1.32   104.  5.55     180\n5 39014  car          ios     2019-04-21 23:56:11   1.33   104. 14.1      179\n6 41170  car          ios     2019-04-21 23:56:09   1.32   104. 13.0       73\n# ℹ 4 more variables: accuracy &lt;dbl&gt;, weekday &lt;ord&gt;, start_hr &lt;fct&gt;, day &lt;fct&gt;\n\n\n\n\nReveal Code\nwrite_rds(origin_df, \"Data/rds/origin_df_part0.rds\")\nwrite_rds(destination_df, \"Data/rds/destination_df_part0.rds\")"
  },
  {
    "objectID": "In-Class_Exercise/In-Class_Exercise_02/In-Class_Exercise_02.html#importing-the-prepared-data",
    "href": "In-Class_Exercise/In-Class_Exercise_02/In-Class_Exercise_02.html#importing-the-prepared-data",
    "title": "In-Class Exercise 2: R for Geospatial Data Science",
    "section": "Importing the prepared data",
    "text": "Importing the prepared data"
  },
  {
    "objectID": "In-Class_Exercise/In-Class_Exercise_02/In-Class_Exercise_02.html#converting-the-aspatial-data-to-geospatial-data",
    "href": "In-Class_Exercise/In-Class_Exercise_02/In-Class_Exercise_02.html#converting-the-aspatial-data-to-geospatial-data",
    "title": "In-Class Exercise 2: R for Geospatial Data Science",
    "section": "Converting the aspatial data to geospatial data",
    "text": "Converting the aspatial data to geospatial data\n\n\nReveal Code\norigin_sf &lt;- \n  st_as_sf(origin_df, coords = c(\"rawlng\", \"rawlat\"), crs = 4326) %&gt;%\n  st_transform(crs = 3414)"
  },
  {
    "objectID": "In-Class_Exercise/In-Class_Exercise_02/In-Class_Exercise_02.html#visualising-the-frequency-distribution",
    "href": "In-Class_Exercise/In-Class_Exercise_02/In-Class_Exercise_02.html#visualising-the-frequency-distribution",
    "title": "In-Class Exercise 2: R for Geospatial Data Science",
    "section": "Visualising the frequency distribution",
    "text": "Visualising the frequency distribution\n\n\nReveal Code\nggplot(data=origin_df, \n       aes(x=weekday)) + \n  geom_bar()"
  },
  {
    "objectID": "In-Class_Exercise/In-Class_Exercise_02/In-Class_Exercise_02.html#visualising-the-trip-origins-as-points",
    "href": "In-Class_Exercise/In-Class_Exercise_02/In-Class_Exercise_02.html#visualising-the-trip-origins-as-points",
    "title": "In-Class Exercise 2: R for Geospatial Data Science",
    "section": "Visualising the trip origins as points",
    "text": "Visualising the trip origins as points\n\n\nReveal Code\ntmap_mode(\"plot\")\ntm_shape(origin_sf) +\n  tm_dots()"
  },
  {
    "objectID": "In-Class_Exercise/In-Class_Exercise_02/In-Class_Exercise_02.html#importing-the-map-layer",
    "href": "In-Class_Exercise/In-Class_Exercise_02/In-Class_Exercise_02.html#importing-the-map-layer",
    "title": "In-Class Exercise 2: R for Geospatial Data Science",
    "section": "Importing the map layer",
    "text": "Importing the map layer\n\n\nReveal Code\nmpsz2019 &lt;- st_read(\"Data/DataGov/MasterPlan2019SubzoneBoundaryNoSeaKML.kml\") %&gt;%\n  st_transform(crs = 3414)\n\n\nReading layer `URA_MP19_SUBZONE_NO_SEA_PL' from data source \n  `C:\\Users\\yungq\\Desktop\\SMU Modules\\Y4S1\\Geospatial Analysis and Applications\\IS415 Course Website\\In-Class_Exercise\\In-Class_Exercise_02\\Data\\DataGov\\MasterPlan2019SubzoneBoundaryNoSeaKML.kml' \n  using driver `KML'\nSimple feature collection with 332 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY, XYZ\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "In-Class_Exercise/In-Class_Exercise_02/In-Class_Exercise_02.html#imposing-the-grab-trip-data-onto-the-map-polygon",
    "href": "In-Class_Exercise/In-Class_Exercise_02/In-Class_Exercise_02.html#imposing-the-grab-trip-data-onto-the-map-polygon",
    "title": "In-Class Exercise 2: R for Geospatial Data Science",
    "section": "Imposing the Grab trip data onto the map polygon",
    "text": "Imposing the Grab trip data onto the map polygon\n\n\nReveal Code\ntm_shape(mpsz2019) + \n  tm_polygons() + \n  tm_shape(origin_sf) +\n  tm_dots()"
  },
  {
    "objectID": "Hands-On_Exercise/Hands-On_Exercise_06/Hands-On_Exercise_06.html",
    "href": "Hands-On_Exercise/Hands-On_Exercise_06/Hands-On_Exercise_06.html",
    "title": "Hands-On Exercise 6:",
    "section": "",
    "text": "Reveal Code\nplot(mtcars)\n\n\n\n\n\nCaption for first plot\n\n\n\n\nReveal Code\nplot(cars)\n\n\n\n\n\nCaption for second plot\n\n\n\n\n\n\n\nCaption for first plot\nCaption for second plot"
  },
  {
    "objectID": "In-Class_Exercise/In-Class_Exercise_05/In-Class_Exercise_05.html#global-and-local-morans-i-analysis",
    "href": "In-Class_Exercise/In-Class_Exercise_05/In-Class_Exercise_05.html#global-and-local-morans-i-analysis",
    "title": "In-Class Exercise 5: Global and Local Measures of Spatial Autocorrelation using sfDep methods",
    "section": "Global and Local Moran’s I Analysis",
    "text": "Global and Local Moran’s I Analysis\n\nDeriving the congruity weights using the QUEEN’s method\n\n\nReveal Code\nwm_q &lt;- hunan_GDPPC %&gt;% \n  mutate(nb = st_contiguity(geometry), wt = st_weights(nb, style = \"W\"), .before = 1)\n\n\n\n\nComputing the Global Moran’s I statistic\n\n\nReveal Code\nmoranI &lt;- global_moran(wm_q$GDPPC, wm_q$nb, wm_q$wt)\n\nglimpse(moranI)\n\n\nList of 2\n $ I: num 0.301\n $ K: num 7.64\n\n\n\n\nPerforming the Global Moran’s I test\n\n\nReveal Code\nglobal_moran_test(wm_q$GDPPC, wm_q$nb, wm_q$wt)\n\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\n\nPerforming the Global Moran’s I permutation test\n\n\nReveal Code\nglobal_moran_perm(x = wm_q$GDPPC, nb = wm_q$nb, wt = wm_q$wt, nsim = 999)\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 999, p-value = 0.002\nalternative hypothesis: two.sided\n\n\n\n\nComputing the Local Moran’s I Statistic\n\n\nReveal Code\nlisa &lt;- wm_q %&gt;% \n  mutate(local_moran = local_moran(GDPPC, nb, wt, nsim = 999), .before = 1) %&gt;%\n  unnest(local_moran)\n\nglimpse(lisa)\n\n\nRows: 88\nColumns: 21\n$ ii           &lt;dbl&gt; -1.468468e-03, 2.587817e-02, -1.198765e-02, 1.022468e-03,…\n$ eii          &lt;dbl&gt; 1.433282e-05, -3.790366e-04, 5.482084e-03, 6.493494e-05, …\n$ var_ii       &lt;dbl&gt; 5.155540e-04, 1.022994e-02, 1.089858e-01, 5.451345e-06, 1…\n$ z_ii         &lt;dbl&gt; -0.06530488, 0.25960440, -0.05291774, 0.41011136, 0.40982…\n$ p_ii         &lt;dbl&gt; 0.9479312534, 0.7951689453, 0.9577974516, 0.6817242595, 0…\n$ p_ii_sim     &lt;dbl&gt; 0.820, 0.926, 0.876, 0.596, 0.580, 0.788, 0.050, 0.110, 0…\n$ p_folded_sim &lt;dbl&gt; 0.410, 0.463, 0.438, 0.298, 0.290, 0.394, 0.025, 0.055, 0…\n$ skewness     &lt;dbl&gt; -0.8746918, -1.0109108, 0.8344852, 1.0626077, 1.0830373, …\n$ kurtosis     &lt;dbl&gt; 0.5472523, 1.4929719, 0.6632058, 1.3826526, 1.2643234, 0.…\n$ mean         &lt;fct&gt; Low-High, Low-Low, High-Low, High-High, High-High, High-L…\n$ median       &lt;fct&gt; High-High, High-High, High-High, High-High, High-High, Hi…\n$ pysal        &lt;fct&gt; Low-High, Low-Low, High-Low, High-High, High-High, High-L…\n$ nb           &lt;nb&gt; &lt;2, 3, 4, 57, 85&gt;, &lt;1, 57, 58, 78, 85&gt;, &lt;1, 4, 5, 85&gt;, &lt;1,…\n$ wt           &lt;list&gt; &lt;0.2, 0.2, 0.2, 0.2, 0.2&gt;, &lt;0.2, 0.2, 0.2, 0.2, 0.2&gt;, &lt;0…\n$ NAME_2       &lt;chr&gt; \"Changde\", \"Changde\", \"Changde\", \"Changde\", \"Changde\", \"C…\n$ ID_3         &lt;int&gt; 21098, 21100, 21101, 21102, 21103, 21104, 21109, 21110, 2…\n$ NAME_3       &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", …\n$ ENGTYPE_3    &lt;chr&gt; \"County\", \"County\", \"County City\", \"County\", \"County\", \"C…\n$ County       &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", …\n$ GDPPC        &lt;dbl&gt; 23667, 20981, 34592, 24473, 25554, 27137, 63118, 62202, 7…\n$ geometry     &lt;POLYGON [°]&gt; POLYGON ((112.0625 29.75523..., POLYGON ((112.228…\n\n\n\n\nVisualising the Local Moran’s I Statistic\n\n\nReveal Code\ntmap_mode(\"plot\")\n\ntm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of GDPPC\",\n            main.title.size = 0.8)\n\n\n\n\n\n\n\n\n\n\n\nVisualising the Local Moran’s I p-value\n\n\nReveal Code\ntmap_mode(\"plot\")\n\ntm_shape(lisa) +\n  tm_fill(\"p_ii_sim\") + \n  tm_borders(alpha = 0.5) +\n   tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 0.8)\n\n\n\n\n\n\n\n\n\n\n\nVisualising the LISA map\n\n\nReveal Code\nlisa_sig &lt;- lisa  %&gt;%\n  filter(p_ii &lt; 0.05)\n\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)"
  },
  {
    "objectID": "In-Class_Exercise/In-Class_Exercise_05/In-Class_Exercise_05.html#hot-cold-spot-analysis",
    "href": "In-Class_Exercise/In-Class_Exercise_05/In-Class_Exercise_05.html#hot-cold-spot-analysis",
    "title": "In-Class Exercise 5: Global and Local Measures of Spatial Autocorrelation using sfDep methods",
    "section": "Hot & Cold Spot Analysis",
    "text": "Hot & Cold Spot Analysis\n\nComputing the Local Gi* Statistic\n\n\nReveal Code\nwm_idw &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\nHCSA &lt;- wm_idw %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    GDPPC, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\n\nglimpse(HCSA)\n\n\nRows: 88\nColumns: 17\n$ gi_star      &lt;dbl&gt; 0.04157939, -0.33349729, 0.28072709, 0.41149401, 0.387293…\n$ e_gi         &lt;dbl&gt; 0.011187352, 0.010966172, 0.012616498, 0.011049479, 0.011…\n$ var_gi       &lt;dbl&gt; 5.824384e-06, 7.424333e-06, 7.623252e-06, 8.259654e-06, 8…\n$ p_value      &lt;dbl&gt; 0.12044798, -0.19088895, -0.14545253, 0.54348412, 0.34317…\n$ p_sim        &lt;dbl&gt; 0.9041282886, 0.8486125961, 0.8843535957, 0.5867965185, 0…\n$ p_folded_sim &lt;dbl&gt; 0.80, 0.98, 0.96, 0.50, 0.58, 0.80, 0.04, 0.08, 0.02, 0.2…\n$ skewness     &lt;dbl&gt; 0.40, 0.49, 0.48, 0.25, 0.29, 0.40, 0.02, 0.04, 0.01, 0.1…\n$ kurtosis     &lt;dbl&gt; 1.1100203, 0.7540941, 0.5091735, 1.1628205, 0.9694879, 0.…\n$ nb           &lt;nb&gt; &lt;2, 3, 4, 57, 85&gt;, &lt;1, 57, 58, 78, 85&gt;, &lt;1, 4, 5, 85&gt;, &lt;1,…\n$ wts          &lt;list&gt; &lt;0.01526149, 0.03515537, 0.02176677, 0.02836978, 0.01029…\n$ NAME_2       &lt;chr&gt; \"Changde\", \"Changde\", \"Changde\", \"Changde\", \"Changde\", \"C…\n$ ID_3         &lt;int&gt; 21098, 21100, 21101, 21102, 21103, 21104, 21109, 21110, 2…\n$ NAME_3       &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", …\n$ ENGTYPE_3    &lt;chr&gt; \"County\", \"County\", \"County City\", \"County\", \"County\", \"C…\n$ County       &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", …\n$ GDPPC        &lt;dbl&gt; 23667, 20981, 34592, 24473, 25554, 27137, 63118, 62202, 7…\n$ geometry     &lt;POLYGON [°]&gt; POLYGON ((112.0625 29.75523..., POLYGON ((112.228…\n\n\n\n\nVisualisng the Local Gi* Statistic\n\n\nReveal Code\ntmap_mode(\"plot\")\n\ntm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8))\n\n\n\n\n\n\n\n\n\n\n\nVisualising the Local Gi* p-value\n\n\nReveal Code\ntmap_mode(\"plot\")\n\ntm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8))\n\n\n\n\n\n\n\n\n\n\n\nVisualising the Hot & Cold Spot Areas\n\n\nReveal Code\nHCSA_sig &lt;- HCSA  %&gt;%\n  filter(p_sim &lt; 0.05)\n\ntmap_mode(\"plot\")\n\ntm_shape(HCSA) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(HCSA_sig) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.4)"
  },
  {
    "objectID": "In-Class_Exercise/In-Class_Exercise_05/In-Class_Exercise_05.html#emerging-hot-cold-spot-analysis",
    "href": "In-Class_Exercise/In-Class_Exercise_05/In-Class_Exercise_05.html#emerging-hot-cold-spot-analysis",
    "title": "In-Class Exercise 5: Global and Local Measures of Spatial Autocorrelation using sfDep methods",
    "section": "Emerging Hot & Cold Spot Analysis",
    "text": "Emerging Hot & Cold Spot Analysis"
  },
  {
    "objectID": "In-Class_Exercise/In-Class_Exercise_05/In-Class_Exercise_05.html#creating-the-spatial-temporal-cube",
    "href": "In-Class_Exercise/In-Class_Exercise_05/In-Class_Exercise_05.html#creating-the-spatial-temporal-cube",
    "title": "In-Class Exercise 5: Global and Local Measures of Spatial Autocorrelation using sfDep methods",
    "section": "Creating the Spatial-Temporal Cube",
    "text": "Creating the Spatial-Temporal Cube\n\n\nReveal Code\nGDPPC_st &lt;- spacetime(hunanGDPPC, hunan,\n                      .loc_col = \"County\",\n                      .time_col = \"Year\")\n\nis_spacetime_cube(GDPPC_st)\n\n\n[1] TRUE\n\n\n\nComputing the Local Gi* Statistic\n\nDeriving the Spatial Weights\n\n\nReveal Code\nGDPPC_nb &lt;- GDPPC_st %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale = 1,\n                                  alpha = 1),\n         .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\n\n\n\nComputing the Gi* Statistic\n\n\nReveal Code\ngi_stars &lt;- GDPPC_nb %&gt;% \n  group_by(Year) %&gt;% \n  mutate(gi_star = local_gstar_perm(\n    GDPPC, nb, wt)) %&gt;% \n  tidyr::unnest(gi_star)\n\n\n\n\n\nConducting the Mann-Kendall Test\n\n\nReveal Code\ncbg &lt;- gi_stars %&gt;% \n  ungroup() %&gt;% \n  filter(County == \"Changsha\") |&gt; \n  select(County, Year, gi_star)\n\n\n\n\nVisualising the Mann-Kendall Test Results\n\n\nReveal Code\nggplot(data = cbg, \n       aes(x = Year, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\n\n\n\n\n\n\n\n\n\n\nVisualising the Mann-Kendall Test Result Interactively\n\n\nReveal Code\np &lt;- ggplot(data = cbg, \n       aes(x = Year, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)\n\n\n\n\n\n\n\n\nComputing the Mann-Kendall p-value to identify emerging hot-spots\n\n\nReveal Code\nehsa &lt;- gi_stars %&gt;%\n  group_by(County) %&gt;%\n  summarise(mk = list(unclass(MannKendall(gi_star)))) %&gt;% \n      unnest_wider(mk)\n\nemerging &lt;- ehsa %&gt;% \n  arrange(sl, abs(tau)) %&gt;% \n  slice(1:5)\n\nemerging\n\n\n# A tibble: 5 × 6\n  County        tau         sl     S     D  varS\n  &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Shuangfeng  0.868 0.00000143   118  136.  589.\n2 Xiangtan    0.868 0.00000143   118  136.  589.\n3 Xiangxiang  0.868 0.00000143   118  136.  589.\n4 Chengbu    -0.824 0.00000482  -112  136.  589.\n5 Dongan     -0.824 0.00000482  -112  136.  589.\n\n\n\n\nPerforming the emerging hot-spot analysis\n\n\nReveal Code\nehsa &lt;- emerging_hotspot_analysis(\n  x = GDPPC_st, \n  .var = \"GDPPC\", \n  k = 1, \n  nsim = 99\n)\n\n\n\n\nVisualising the emerging hotspot classes\n\n\nReveal Code\nggplot(data = ehsa,\n       aes(x = classification)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\nVisualising the emerging hot-spots geospatially\n\n\nReveal Code\nhunan_ehsa &lt;- hunan %&gt;%\n  left_join(ehsa,\n            by = join_by(County == location))\n\nehsa_sig &lt;- hunan_ehsa  %&gt;%\n  filter(p_value &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(hunan_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\") + \n  tm_borders(alpha = 0.4)"
  },
  {
    "objectID": "Take-Home_Exercise/Take-Home_Exercise_02/Take-Home_Exercise_02.html",
    "href": "Take-Home_Exercise/Take-Home_Exercise_02/Take-Home_Exercise_02.html",
    "title": "Take-Home Exercise 2: Using Spatial-Temporal Analysis to investigate the distribution of Dengue Fever cases in Tainan City, Taiwan",
    "section": "",
    "text": "As mentioned in the project outline, Taiwan has been very successful in containing the spread of Dengue Fever for many years. However, in 2023, especially between the epidemic weeks 31 to 50 (which corresponds to the end of July to the middle of December), Taiwan saw a dramatic spike in Dengue Fever cases. The interest of this report then, is to conduct an analysis into the cause of the outbreak, specifically in Tainan City, Taiwan, from a spatial autocorrelation perspective."
  },
  {
    "objectID": "Take-Home_Exercise/Take-Home_Exercise_02/Data/Geospatial/TAINAN_VILLAGE.html",
    "href": "Take-Home_Exercise/Take-Home_Exercise_02/Data/Geospatial/TAINAN_VILLAGE.html",
    "title": "IS415 Course Website",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“TWD97”,DATUM[“Taiwan Datum 1997”,ELLIPSOID[“GRS 1980”,6378137,298.257222101,LENGTHUNIT[“metre”,1]]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“Taiwan, Republic of China - onshore and offshore - Taiwan Island, Penghu (Pescadores) Islands.”],BBOX[17.36,114.32,26.96,123.61]],ID[“EPSG”,3824]] +proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs 27230 3824 EPSG:3824 TWD97 longlat EPSG:7019 true"
  },
  {
    "objectID": "Take-Home_Exercise/Take-Home_Exercise_02/Take-Home_Exercise_02.html#priming-the-project",
    "href": "Take-Home_Exercise/Take-Home_Exercise_02/Take-Home_Exercise_02.html#priming-the-project",
    "title": "Take-Home Exercise 2: Using Spatial-Temporal Analysis to investigate the distribution of Dengue Fever cases in Tainan City, Taiwan",
    "section": "",
    "text": "As mentioned in the project outline, Taiwan has been very successful in containing the spread of Dengue Fever for many years. However, in 2023, especially between the epidemic weeks 31 to 50 (which corresponds to the end of July to the middle of December), Taiwan saw a dramatic spike in Dengue Fever cases. The interest of this report then, is to conduct an analysis into the cause of the outbreak, specifically in Tainan City, Taiwan, from a spatial autocorrelation perspective."
  },
  {
    "objectID": "Take-Home_Exercise/Take-Home_Exercise_02/Take-Home_Exercise_02.html#preparing-the-r-environment-for-analysis",
    "href": "Take-Home_Exercise/Take-Home_Exercise_02/Take-Home_Exercise_02.html#preparing-the-r-environment-for-analysis",
    "title": "Take-Home Exercise 2: Using Spatial-Temporal Analysis to investigate the distribution of Dengue Fever cases in Tainan City, Taiwan",
    "section": "Preparing the R-Environment for analysis",
    "text": "Preparing the R-Environment for analysis\n\nLoading the necessary R-packages\nTo do so, we will need the following R-packages:\n\nsf: Allows us to handle both spatial data, i.e. the Tainan map data and XY coordinates within the dengue dataset.\nsfdep: Allows us to run spatial autocorrelation statistical analysis of our data.\nKendall: Used to run the Mann-Kendall’s test in emerging hotspot analysis\ntmap: For visualisation of the spatial data.\ntidyverse: To allow us to investigate, manipulate and write the datasets of interest in a readable manner. Also has great compatibility with sf objects.\nlubridate: To handle time format data and conversion of other data types into time formats.\nggplot2: Used to visualise data in forms like histograms and line graphs.\nhtmlwidgets: Used to save interactive spatial data visualisations as HTML widgets to lessen the burden on Quarto when rendering.\n\nThe p_load() functionfrom the pacman package is used to load the libraries and automatically install them for the user if they aren’t already installed.\n\n\nReveal Code\npacman::p_load(sf, sfdep, Kendall, tmap, tidyverse, lubridate, ggplot2, htmlwidgets)"
  },
  {
    "objectID": "Take-Home_Exercise/Take-Home_Exercise_02/Take-Home_Exercise_02.html#conducting-data-pre-processing-for-analysis",
    "href": "Take-Home_Exercise/Take-Home_Exercise_02/Take-Home_Exercise_02.html#conducting-data-pre-processing-for-analysis",
    "title": "Take-Home Exercise 2: Using Spatial-Temporal Analysis to investigate the distribution of Dengue Fever cases in Tainan City, Taiwan",
    "section": "Conducting data pre-processing for analysis",
    "text": "Conducting data pre-processing for analysis\nThere are 2 datasets of interest to us in this report, the Tainan spatial data, and the dengue_daily aspatial data. The Tainan dataset contains spatial polygon information about the geographical boundaries of Tainan City, whilst the dengue_daily dataset is a record of all reported instances of dengue fever in Taiwan from 1998 till 2023.\n\nImporting the datasets of interest\n\nTainan Spatial Data:\nTo import the Tainan dataset, we require the the st_read() function from the sf package so as to preserve its spatial features. Additionally, st_transform() is required in order to cast the data set into the WGS84 coordinate reference system as the data is originally in TWD97, the Taiwanese CRS. We need to do so in order to visualise the data later on upon OpenStreetMap later as OpenStreetMap is coded in WGS84. From the glimpse() of the data, we can see that we were successful in doing so.\n\n\nReveal Code\ntainan &lt;- st_read(dsn = \"Data/Geospatial\", layer = \"TAINAN_VILLAGE\") %&gt;% \n  st_transform(4326)\n\n\nReading layer `TAINAN_VILLAGE' from data source \n  `C:\\Users\\yungq\\Desktop\\SMU Modules\\Y4S1\\Geospatial Analysis and Applications\\IS415 Course Website\\Take-Home_Exercise\\Take-Home_Exercise_02\\Data\\Geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 649 features and 10 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 120.0269 ymin: 22.88751 xmax: 120.6563 ymax: 23.41374\nGeodetic CRS:  TWD97\n\n\nReveal Code\nsummary(tainan)\n\n\n   VILLCODE          COUNTYNAME          TOWNNAME           VILLNAME        \n Length:649         Length:649         Length:649         Length:649        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n   VILLENG            COUNTYID          COUNTYCODE           TOWNID         \n Length:649         Length:649         Length:649         Length:649        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n   TOWNCODE             NOTE                    geometry  \n Length:649         Length:649         POLYGON      :649  \n Class :character   Class :character   epsg:4326    :  0  \n Mode  :character   Mode  :character   +proj=long...:  0  \n\n\nReveal Code\nglimpse(tainan)\n\n\nRows: 649\nColumns: 11\n$ VILLCODE   &lt;chr&gt; \"67000280002\", \"67000350032\", \"67000150009\", \"67000150010\",…\n$ COUNTYNAME &lt;chr&gt; \"臺南市\", \"臺南市\", \"臺南市\", \"臺南市\", \"臺南市\", \"臺南市\",…\n$ TOWNNAME   &lt;chr&gt; \"歸仁區\", \"安南區\", \"七股區\", \"七股區\", \"七股區\", \"七股區\",…\n$ VILLNAME   &lt;chr&gt; \"六甲里\", \"青草里\", \"溪南里\", \"七股里\", \"龍山里\", \"中寮里\",…\n$ VILLENG    &lt;chr&gt; \"Liujia Vil.\", \"Qingcao Vil.\", \"Xinan Vil.\", \"Qigu Vil.\", \"…\n$ COUNTYID   &lt;chr&gt; \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\",…\n$ COUNTYCODE &lt;chr&gt; \"67000\", \"67000\", \"67000\", \"67000\", \"67000\", \"67000\", \"6700…\n$ TOWNID     &lt;chr&gt; \"D33\", \"D06\", \"D22\", \"D22\", \"D22\", \"D22\", \"D22\", \"D22\", \"D2…\n$ TOWNCODE   &lt;chr&gt; \"67000280\", \"67000350\", \"67000150\", \"67000150\", \"67000150\",…\n$ NOTE       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ geometry   &lt;POLYGON [°]&gt; POLYGON ((120.2725 22.95868..., POLYGON ((120.1176 …\n\n\nPlotting the Tainan geospatial data with tmap’s tm_polygons() function for a quick visualisation (and confirmation of the data):\n\n\nReveal Code\ntm_shape(tainan) +\n  tm_polygons(col = \"darkgreen\", border.col = \"black\") + \n  tm_layout(main.title = \"Map of Tainan, Taiwan\", main.title.size = 0.9, main.title.position = \"center\", bg.color = \"lightblue\") +\n  tm_compass(type = \"rose\", size = 3) +\n  tm_scale_bar(width = 0.2, )\n\n\n\n\n\n\n\n\n\n\n\nDaily Dengue report data:\nNext, we will import the dengue_daily dataset, which is in csv (comma-separated values) format. We will rely on the read_csv() function from the readr package which is part of the tidyverse repository to do so. Additionally, we will not need the sf package for now as the spatial information in the dataset is in character format and thus, will not be recognised automatically by st_read() anyways:\n\n\nReveal Code\ndengue &lt;- read_csv(\"Data/Aspatial/Dengue_Daily.csv\")\nglimpse(dengue)\n\n\nRows: 106,861\nColumns: 26\n$ 發病日             &lt;date&gt; 1998-01-02, 1998-01-03, 1998-01-13, 1998-01-15, 19…\n$ 個案研判日         &lt;chr&gt; \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"No…\n$ 通報日             &lt;date&gt; 1998-01-07, 1998-01-14, 1998-02-18, 1998-01-23, 19…\n$ 性別               &lt;chr&gt; \"男\", \"男\", \"男\", \"男\", \"男\", \"男\", \"男\", \"女\", \"女…\n$ 年齡層             &lt;chr&gt; \"40-44\", \"30-34\", \"55-59\", \"35-39\", \"55-59\", \"20-24…\n$ 居住縣市           &lt;chr&gt; \"屏東縣\", \"屏東縣\", \"宜蘭縣\", \"高雄市\", \"宜蘭縣\", \"…\n$ 居住鄉鎮           &lt;chr&gt; \"屏東市\", \"東港鎮\", \"宜蘭市\", \"苓雅區\", \"五結鄉\", \"…\n$ 居住村里           &lt;chr&gt; \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"No…\n$ 最小統計區         &lt;chr&gt; \"A1320-0136-00\", \"A1303-0150-00\", \"A0201-0449-00\", …\n$ 最小統計區中心點X  &lt;chr&gt; \"120.505898941\", \"120.453657460\", \"121.751433765\", …\n$ 最小統計區中心點Y  &lt;chr&gt; \"22.464206650\", \"22.466338948\", \"24.749214667\", \"22…\n$ 一級統計區         &lt;chr&gt; \"A1320-04-008\", \"A1303-09-007\", \"A0201-23-006\", \"A6…\n$ 二級統計區         &lt;chr&gt; \"A1320-04\", \"A1303-09\", \"A0201-23\", \"A6408-10\", \"A0…\n$ 感染縣市           &lt;chr&gt; \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"No…\n$ 感染鄉鎮           &lt;chr&gt; \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"No…\n$ 感染村里           &lt;chr&gt; \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"No…\n$ 是否境外移入       &lt;chr&gt; \"否\", \"是\", \"是\", \"否\", \"否\", \"是\", \"否\", \"否\", \"是…\n$ 感染國家           &lt;chr&gt; \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"No…\n$ 確定病例數         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ 居住村里代碼       &lt;chr&gt; \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"No…\n$ 感染村里代碼       &lt;chr&gt; \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"No…\n$ 血清型             &lt;chr&gt; \"None\", \"第二型\", \"None\", \"None\", \"None\", \"None\", \"…\n$ 內政部居住縣市代碼 &lt;chr&gt; \"10013\", \"10013\", \"10002\", \"64\", \"10002\", \"68\", \"65…\n$ 內政部居住鄉鎮代碼 &lt;chr&gt; \"1001301\", \"1001303\", \"1000201\", \"6400800\", \"100020…\n$ 內政部感染縣市代碼 &lt;chr&gt; \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"No…\n$ 內政部感染鄉鎮代碼 &lt;chr&gt; \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"No…\n\n\nA quick investigation of the data yields the following:\n\nThere are missing spatial information in the dataset (some reports do not list reporting coordinates).\nIt is a medium sized dataset with 106, 861 rows.\n\nA quick calculation reveals that there are 1560 rows with missing coordinate information. Considering the overall size of the data, it is reasonable, nor do we have much option anyways, to remove rows with missing spatial information from the dataset as that will have repurcussions on our analysis if we leave them be.\nTo do so, we rely upon the mutate() and filter() functions from dplyr(), whilst using as.numeric() to convert the coordinates into floating number values so that sf may recognise it. To convert the dataset into a simple features object, we explicitly parse the “最小統計區中心點X” and “最小統計區中心點Y” columns as spatial information, using the st_as_st() function to cast the dataframe, and the st_set_crs() function to specify the use of the WGS84 coordinate reference system.\n\n\nReveal Code\ndengue &lt;- dengue %&gt;% \n  mutate(最小統計區中心點X = as.numeric(最小統計區中心點X),\n         最小統計區中心點Y = as.numeric(最小統計區中心點Y)) %&gt;%\n  filter(!is.na(最小統計區中心點X) & !is.na(最小統計區中心點Y)) %&gt;% \n  st_as_sf(coords = c(\"最小統計區中心點X\", \"最小統計區中心點Y\"), crs = \"TWD97\") %&gt;% \n  st_set_crs(4326)\n\nsummary(dengue)\n\n\n     發病日            個案研判日            通報日          \n Min.   :1998-01-02   Length:106081      Min.   :1998-01-07  \n 1st Qu.:2014-10-29   Class :character   1st Qu.:2014-11-01  \n Median :2015-09-29   Mode  :character   Median :2015-10-01  \n Mean   :2016-04-09                      Mean   :2016-04-12  \n 3rd Qu.:2023-07-02                      3rd Qu.:2023-07-05  \n Max.   :2024-01-29                      Max.   :2024-01-29  \n     性別              年齡層            居住縣市           居住鄉鎮        \n Length:106081      Length:106081      Length:106081      Length:106081     \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   居住村里          最小統計區         一級統計區         二級統計區       \n Length:106081      Length:106081      Length:106081      Length:106081     \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   感染縣市           感染鄉鎮           感染村里         是否境外移入      \n Length:106081      Length:106081      Length:106081      Length:106081     \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   感染國家           確定病例數 居住村里代碼       感染村里代碼      \n Length:106081      Min.   :1    Length:106081      Length:106081     \n Class :character   1st Qu.:1    Class :character   Class :character  \n Mode  :character   Median :1    Mode  :character   Mode  :character  \n                    Mean   :1                                         \n                    3rd Qu.:1                                         \n                    Max.   :2                                         \n    血清型          內政部居住縣市代碼 內政部居住鄉鎮代碼 內政部感染縣市代碼\n Length:106081      Length:106081      Length:106081      Length:106081     \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n 內政部感染鄉鎮代碼          geometry     \n Length:106081      POINT        :106081  \n Class :character   epsg:4326    :     0  \n Mode  :character   +proj=long...:     0  \n                                          \n                                          \n                                          \n\n\nReveal Code\nglimpse(dengue)\n\n\nRows: 106,081\nColumns: 25\n$ 發病日             &lt;date&gt; 1998-01-02, 1998-01-03, 1998-01-13, 1998-01-15, 19…\n$ 個案研判日         &lt;chr&gt; \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"No…\n$ 通報日             &lt;date&gt; 1998-01-07, 1998-01-14, 1998-02-18, 1998-01-23, 19…\n$ 性別               &lt;chr&gt; \"男\", \"男\", \"男\", \"男\", \"男\", \"男\", \"女\", \"女\", \"男…\n$ 年齡層             &lt;chr&gt; \"40-44\", \"30-34\", \"55-59\", \"35-39\", \"55-59\", \"40-44…\n$ 居住縣市           &lt;chr&gt; \"屏東縣\", \"屏東縣\", \"宜蘭縣\", \"高雄市\", \"宜蘭縣\", \"…\n$ 居住鄉鎮           &lt;chr&gt; \"屏東市\", \"東港鎮\", \"宜蘭市\", \"苓雅區\", \"五結鄉\", \"…\n$ 居住村里           &lt;chr&gt; \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"No…\n$ 最小統計區         &lt;chr&gt; \"A1320-0136-00\", \"A1303-0150-00\", \"A0201-0449-00\", …\n$ 一級統計區         &lt;chr&gt; \"A1320-04-008\", \"A1303-09-007\", \"A0201-23-006\", \"A6…\n$ 二級統計區         &lt;chr&gt; \"A1320-04\", \"A1303-09\", \"A0201-23\", \"A6408-10\", \"A0…\n$ 感染縣市           &lt;chr&gt; \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"No…\n$ 感染鄉鎮           &lt;chr&gt; \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"No…\n$ 感染村里           &lt;chr&gt; \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"No…\n$ 是否境外移入       &lt;chr&gt; \"否\", \"是\", \"是\", \"否\", \"否\", \"否\", \"否\", \"是\", \"是…\n$ 感染國家           &lt;chr&gt; \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"No…\n$ 確定病例數         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ 居住村里代碼       &lt;chr&gt; \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"No…\n$ 感染村里代碼       &lt;chr&gt; \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"No…\n$ 血清型             &lt;chr&gt; \"None\", \"第二型\", \"None\", \"None\", \"None\", \"None\", \"…\n$ 內政部居住縣市代碼 &lt;chr&gt; \"10013\", \"10013\", \"10002\", \"64\", \"10002\", \"65\", \"63…\n$ 內政部居住鄉鎮代碼 &lt;chr&gt; \"1001301\", \"1001303\", \"1000201\", \"6400800\", \"100020…\n$ 內政部感染縣市代碼 &lt;chr&gt; \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"No…\n$ 內政部感染鄉鎮代碼 &lt;chr&gt; \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"No…\n$ geometry           &lt;POINT [°]&gt; POINT (120.5059 22.46421), POINT (120.4537 22…\n\n\nAs the dengue information covers the whole of Taiwan, the st_intersection() function is used to confine the spread of the data solely to Tainan, our project objective. st_union() is applied here to draw the outer boundaries of Tainan, and speed up the computation time.\n\n\nReveal Code\ndengue &lt;- dengue %&gt;% \n  st_intersection(st_union(tainan))\n\n\n\n\nReveal Code\ndengue %&gt;% \n  write_rds(\"Data/RDS/dengue_tainan.rds\")\n\n\nA visualisation of the locations of dengue fever reports in Tainan is as follows:\n\n\nReveal Code\ntm_shape(tainan) + \n  tm_polygons(col = \"darkgreen\", border.col = \"black\") + \n  tm_layout(main.title = \"Outbreak of Dengue Fever in Tainan, Taiwan\", main.title.size = 0.9, main.title.position = \"center\", bg.color = \"lightblue\") +\ntm_shape(dengue) + \n  tm_dots(size = 0.05, alpha = 0.2, col = \"red\") +\n  tm_compass(type = \"rose\", size = 3, position = \"left\") +\n  tm_scale_bar(width = 0.2, position = \"left\")\n\n\n\n\n\n\n\n\n\nFrom the incident map drawn, we can see that the majority of dengue fever reports occur in the south-western part of the city which explains why a further restriction of the data to a couple of towns within Tainan was specified as part of the report. Below, we will constrict the Tainan spatial polygon data to the requested towns using the filter() function from dpylr, and produce an easy visualisation of the new area of focus.\n\n\nReveal Code\ntainan &lt;- tainan %&gt;% \n  filter(TOWNID %in% c(\"D01\", \"D02\", \"D04\", \"D06\", \"D07\", \"D08\", \"D32\", \"D39\"))\nglimpse(tainan)\n\n\nRows: 258\nColumns: 11\n$ VILLCODE   &lt;chr&gt; \"67000350032\", \"67000270011\", \"67000370005\", \"67000330004\",…\n$ COUNTYNAME &lt;chr&gt; \"臺南市\", \"臺南市\", \"臺南市\", \"臺南市\", \"臺南市\", \"臺南市\",…\n$ TOWNNAME   &lt;chr&gt; \"安南區\", \"仁德區\", \"中西區\", \"南區\", \"安南區\", \"安南區\", \"…\n$ VILLNAME   &lt;chr&gt; \"青草里\", \"保安里\", \"赤嵌里\", \"大成里\", \"城北里\", \"城南里\",…\n$ VILLENG    &lt;chr&gt; \"Qingcao Vil.\", \"Bao'an Vil.\", \"Chihkan Vil.\", \"Dacheng Vil…\n$ COUNTYID   &lt;chr&gt; \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\",…\n$ COUNTYCODE &lt;chr&gt; \"67000\", \"67000\", \"67000\", \"67000\", \"67000\", \"67000\", \"6700…\n$ TOWNID     &lt;chr&gt; \"D06\", \"D32\", \"D08\", \"D02\", \"D06\", \"D06\", \"D08\", \"D06\", \"D0…\n$ TOWNCODE   &lt;chr&gt; \"67000350\", \"67000270\", \"67000370\", \"67000330\", \"67000350\",…\n$ NOTE       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ geometry   &lt;POLYGON [°]&gt; POLYGON ((120.1176 23.08387..., POLYGON ((120.2304 …\n\n\nReveal Code\ntm_shape(tainan) +\n  tm_polygons(col = \"darkgreen\", border.col = \"black\") + \n  tm_layout(main.title = \"Outbreak Region of Tainan, Taiwan\", main.title.size = 0.9, main.title.position = \"center\", bg.color = \"lightblue\") +\n  tm_compass(type = \"rose\", size = 3, position = \"left\") +\n  tm_scale_bar(width = 0.2, position = \"left\")\n\n\n\n\n\n\n\n\n\nAs the boundary of the reports have been re-drawn, we will need to employ the st_intersection() function once again to restrict the dengue reports to the focus area:\n\n\nReveal Code\ndengue &lt;- dengue %&gt;% \n  st_intersection(st_union(tainan))\n\n\nBy plotting the newly restricted daily_dengue data over the Tainan data subset, we can confirm that the restriction process has gone well:\n\n\nReveal Code\ntm_shape(tainan) + \n  tm_polygons(col = \"darkgreen\", border.col = \"black\") + \n  tm_layout(main.title = \"Cases of Dengue Fever in Tainan, Taiwan\", main.title.size = 0.9, main.title.position = \"center\", bg.color = \"lightblue\") +\ntm_shape(dengue) + \n  tm_dots(size = 0.05, alpha = 0.2, col = \"red\") +\n  tm_compass(type = \"rose\", size = 3, position = \"left\") +\n  tm_scale_bar(width = 0.2, position = \"left\")\n\n\n\n\n\n\n\n\n\nNext, we will need to reduce the dengue report data that we have to the time period which we are interested in, namely the 31st to 50th epidemic weeks of 2023. To do so, we first need to extract out the year and week information from the data. Thus, we apply the year() and isoweek() functions from the lubridate package to do so, and the mutate() function from dplyr to create these new columns in the data.\nThere are 3 functions to extract the week information from a date field in lubridate, but we will use isoweek() for the following reason. In order to conduct year-on-year analysis of the data, we will need a way to standardise the weeks extracted across the years. weeks() will not allow us to do so, whereas isoweek() will. Of course as we are not doing year-on-year analysis, choosing isoweek() will not make an impactful difference, but it still is good practice to do so. epiweek() is discounted on the other hand, as even though it standardises week extraction, it starts the week on a Sunday rather than a Monday as is standard in Taiwan.\n\n\nReveal Code\ndengue &lt;- dengue %&gt;% \n  mutate(發病年 = year(發病日), 發病周 = isoweek(發病日)) \n\ndengue %&gt;% \n  group_by(發病周) %&gt;% \n  summarise(dengue_count = n()) %&gt;% \n    ggplot(aes(y = dengue_count, x = 發病周)) +\n    geom_line(col = \"orange\", scale_colour_hue = 100) +\n    labs(title = \"Trend of Dengue Fever Cases in Tainan, Taiwan\") +\n    xlab(\"Epidemic Week\") +\n    ylab(\"Number of Dengue Cases Reported\") + \n    theme(panel.background = element_rect(fill = \"navy\"), \n          panel.grid.major = element_line(color = \"black\", linewidth = 0.1),\n          panel.grid.minor = element_line(color = \"black\", linewidth = 0.1))\n\n\n\n\n\n\n\n\n\nUsing ggplot2 this time, we visualise above, the trend of dengue reports across our area of focus in Taiwan over 2023 on a weekly basis. From our graph, it becomes evident that the dengue reports does pick up dramatically from weeks 31 to 50 as the project outline states. Thus, we employ the filter() function once again to narrow down our study scope.\n\n\nReveal Code\ndengue &lt;- dengue %&gt;%\n  filter(發病年 == 2023 & (between(發病周, 31, 50)))\n\n\n\n\nReveal Code\ndengue %&gt;% \n  write_rds(\"Data/RDS/dengue_tainan_wk31_50.rds\")\n\n\nA visualisation of the base of our study using tmap is thus, as follows:\n\n\nReveal Code\ntm_shape(tainan) + \n  tm_polygons(col = \"darkgreen\", border.col = \"black\") + \n  tm_layout(main.title = \"Cases of Dengue Fever between epidemic weeks 31 to 50 in Tainan, Taiwan\", main.title.size = 0.75, main.title.position = \"center\", bg.color = \"lightblue\") +\ntm_shape(dengue) + \n  tm_dots(size = 0.05, alpha = 0.2, col = \"red\") +\n  tm_compass(type = \"rose\", size = 3, position = \"left\") +\n  tm_scale_bar(width = 0.2, position = \"left\")"
  },
  {
    "objectID": "Take-Home_Exercise/Take-Home_Exercise_02/Take-Home_Exercise_02.html#conducting-global-spatial-auto-correlation-analysis",
    "href": "Take-Home_Exercise/Take-Home_Exercise_02/Take-Home_Exercise_02.html#conducting-global-spatial-auto-correlation-analysis",
    "title": "Take-Home Exercise 2: Using Spatial-Temporal Analysis to investigate the distribution of Dengue Fever cases in Tainan City, Taiwan",
    "section": "Conducting Global Spatial Auto-correlation analysis",
    "text": "Conducting Global Spatial Auto-correlation analysis\nTo conduct our global spatial autocorrelation analysis, we will employ Moran’s I statistics rather than Geary’s C as transmission of dengue occurs via the Aedes vector, and we should not lighten the impact of linear associations of dengue reports from our analysis.\nTo start, we need to count the number of reports which occurred in each polygon of our geographical scope. The st_intersects() function is employed over the st_intersection() function here for computation speed as we need not retain all information in the data, only just the counts. lengths() is used to compute the length of each element in the list generated by st_intersects().\n\n\nReveal Code\ntainan$dengue_count &lt;- tainan %&gt;% \n  st_intersects(dengue) %&gt;% \n  lengths()\n\nglimpse(tainan)\n\n\nRows: 258\nColumns: 12\n$ VILLCODE     &lt;chr&gt; \"67000350032\", \"67000270011\", \"67000370005\", \"67000330004…\n$ COUNTYNAME   &lt;chr&gt; \"臺南市\", \"臺南市\", \"臺南市\", \"臺南市\", \"臺南市\", \"臺南市…\n$ TOWNNAME     &lt;chr&gt; \"安南區\", \"仁德區\", \"中西區\", \"南區\", \"安南區\", \"安南區\",…\n$ VILLNAME     &lt;chr&gt; \"青草里\", \"保安里\", \"赤嵌里\", \"大成里\", \"城北里\", \"城南里…\n$ VILLENG      &lt;chr&gt; \"Qingcao Vil.\", \"Bao'an Vil.\", \"Chihkan Vil.\", \"Dacheng V…\n$ COUNTYID     &lt;chr&gt; \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D…\n$ COUNTYCODE   &lt;chr&gt; \"67000\", \"67000\", \"67000\", \"67000\", \"67000\", \"67000\", \"67…\n$ TOWNID       &lt;chr&gt; \"D06\", \"D32\", \"D08\", \"D02\", \"D06\", \"D06\", \"D08\", \"D06\", \"…\n$ TOWNCODE     &lt;chr&gt; \"67000350\", \"67000270\", \"67000370\", \"67000330\", \"67000350…\n$ NOTE         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ geometry     &lt;POLYGON [°]&gt; POLYGON ((120.1176 23.08387..., POLYGON ((120.230…\n$ dengue_count &lt;int&gt; 2, 19, 111, 29, 1, 10, 38, 44, 112, 65, 28, 2, 3, 11, 24,…\n\n\nNext, we need to compute the congruity weight matrix as Moran’s I is a congruity based method. The st_contiguity() is used to do so, with the queen option set to true as mentioned prior, the vector-transmission method of dengue fever does not allow us to discount neither the neighbours at the edge nor vertices. The st_weights() function is used to compute the weights of each polygon in the dataset, with style set to ‘W’ so that row-standardised weights are computed.\n\n\nReveal Code\nweight_matrix_queen &lt;- tainan %&gt;% \n  mutate(neighbours = st_contiguity(geometry, queen = TRUE),\n         weight = st_weights(neighbours, style = \"W\"),\n         .before = 1)\n\nsummary(weight_matrix_queen$neighbours)\n\n\nNeighbour list object:\nNumber of regions: 258 \nNumber of nonzero links: 1526 \nPercentage nonzero weights: 2.29253 \nAverage number of links: 5.914729 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 10 11 12 14 \n 4 17 47 49 49 41 26 14  6  3  1  1 \n4 least connected regions:\n77 117 138 238 with 2 links\n1 most connected region:\n128 with 14 links\n\n\nAfter we obtain the row-standardised weight matrix of our spatial data, we can proceed to compute the global Moran’s statistic with it. global_moran_perm() is used to conduct a Monte-Carlo simulation of the global Moran’s I statistic with 500 simulations done. As in all Monte-Carlo simulations conducted in this report, 500 simulations will be used as it represents the largest amount of simulations my computer can conduct in a reasonable amount of time.\nAdditionally, set.seed() is used to ensure that the results of the simulation are repeatable.\n\n\nReveal Code\nset.seed(3142)\n\nglobal_moran_500 &lt;- global_moran_perm(weight_matrix_queen$dengue_count, \n                  weight_matrix_queen$neighbours,\n                  weight_matrix_queen$weight,\n                  nsim = 499)\n\nprint(global_moran_500)\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 500 \n\nstatistic = 0.46821, observed rank = 500, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nFrom the simulation results, we can see that the p-value is extremely low, leading to a conclusion that the incidents of dengue cases in Tainan show evidence of non-randomness at all reasonable levels of statistical significance. This result is not surprising as Aedes mosquitoes breed and travel short ranges, meaning that when they start carrying the Dengue virus, we expect people near to the initial carrier of dengue to suffer from it to. Also, we might think from the perspective that areas where Aedes mosquitoes thrive will frequently give rise to clusters of dengue cases as they are more prone to breeding of mosquitoes which makes it difficult to eliminate the vector in these areas.\nWe can visualise the Monte-Carlo simulation using geom_histogram() and geom_vline() from ggplot2 as below:\n\n\nReveal Code\nggplot() + \n  geom_histogram(aes(global_moran_500$res[1:499]),\n                 bins = 100, \n                 fill = \"blue\", \n                 color = \"black\", \n                 alpha = 0.7, \n                 size = 0.25) + \n  geom_vline(xintercept = mean(global_moran_500$res[1:499]), \n              color = \"red\") +\n  labs(title = \"Monte-Carlo Simulation of Global Moran's I test-statistic (n = 500)\") +\n  xlab(\"Global Moran's I test-statistic\") +\n  ylab(\"Frequency\") +\n  theme_classic()"
  },
  {
    "objectID": "Take-Home_Exercise/Take-Home_Exercise_02/Take-Home_Exercise_02.html#conducting-local-spatial-auto-correlation-analysis",
    "href": "Take-Home_Exercise/Take-Home_Exercise_02/Take-Home_Exercise_02.html#conducting-local-spatial-auto-correlation-analysis",
    "title": "Take-Home Exercise 2: Using Spatial-Temporal Analysis to investigate the distribution of Dengue Fever cases in Tainan City, Taiwan",
    "section": "Conducting Local Spatial Auto-correlation analysis",
    "text": "Conducting Local Spatial Auto-correlation analysis\nFor the same reasons as in the global spatial auto-correlation analysis, Moran’s I statistic will be used over Geary’s C here.\nLike in our global spatial auto-correlation analysis, we will need a congruity weight matrix first as Moran’s I is a congruity-based method. Luckily, there is no obvious reason to compute a different congruity weight matrix and thus we will used the one generated in our global spatial auto-correlation analysis.\nTo apply the local Moran’s I test, the local_moran() function is used and simulated over 500 times. As the result of the local_moran() function is a nested list, unnest() is thus, needed to expand it into a readable format.\n\n\nReveal Code\nlisa &lt;- weight_matrix_queen %&gt;% \n  mutate(local_moran = local_moran(weight_matrix_queen$dengue_count, \n                  weight_matrix_queen$neighbours,\n                  weight_matrix_queen$weight,\n                  nsim = 499),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\nAn interactive display of the Moran’s I statistic is shown below:\n\n\nReveal Code\nlisa_leaflet &lt;- tm_shape(lisa) +\n  tm_fill(\"ii\", \n          title = \"Local Moran's I\", \n          interactive = TRUE,\n          alpha = 0.3) + \n  tm_borders(alpha = 1) + \n  tm_basemap(\"OpenStreetMap\")\n\nlisa_leaflet &lt;- tmap_leaflet(lisa_leaflet)\n\nsaveWidget(lisa_leaflet, \"Screenshots/lisa_leaflet.html\")\n\n\n\n\n\n\nAn interactive display of the p-values of the Moran’s I statistic is shown below:\n\n\nReveal Code\nlisa_p_leaflet &lt;- tm_shape(lisa) +\n  tm_fill(\"p_ii_sim\", \n          title = \"Local Moran's I p-value\", \n          interactive = TRUE,\n          alpha = 0.3) + \n  tm_borders(alpha = 1) + \n  tm_basemap(\"OpenStreetMap\")\n\nlisa_p_leaflet &lt;- tmap_leaflet(lisa_p_leaflet)\n\nsaveWidget(lisa_p_leaflet, \"Screenshots/lisa_p_leaflet.html\")\n\n\n\n\n\n\nAn interactive display of the regions with a significant Moran’s I statistic (5% level) is shown below:\n\n\nReveal Code\nlisa_sig_leaflet &lt;- lisa %&gt;%\n  tm_shape(filter = .$p_ii &lt;= 0.05) +\n    tm_fill(\"mean\", \n            title = \"Local Moran's I clusters\", \n            interactive = TRUE,\n            alpha = 0.3) + \n    tm_borders(alpha = 1) + \n    tm_basemap(\"OpenStreetMap\")\n\nlisa_sig_leaflet &lt;- tmap_leaflet(lisa_sig_leaflet)\n\nsaveWidget(lisa_sig_leaflet, \"Screenshots/lisa_sig_leaflet.html\")"
  },
  {
    "objectID": "Take-Home_Exercise/Take-Home_Exercise_02/Take-Home_Exercise_02.html#conducting-emerging-hot-spot-analysis",
    "href": "Take-Home_Exercise/Take-Home_Exercise_02/Take-Home_Exercise_02.html#conducting-emerging-hot-spot-analysis",
    "title": "Take-Home Exercise 2: Using Spatial-Temporal Analysis to investigate the distribution of Dengue Fever cases in Tainan City, Taiwan",
    "section": "Conducting Emerging Hot-Spot Analysis",
    "text": "Conducting Emerging Hot-Spot Analysis\nBeyond spatial auto-correlation analysis, we should also be interested in spatio-temporal analysis as the intensity of the outbreak over time will determine how easy is it to contain it. To do so, we will need to create a spacetime_cube object using the spacetime() function in sfdep.\nHowever, before we can proceed to do so, we will need to create a dataframe of all possible combinations of spatio-temporal events across our dataset. This is done via the following code:\n\n\nReveal Code\nspatial_temporal_dengue_count &lt;- crossing(dengue$發病周, \n         tainan$VILLCODE) %&gt;% \n  rename(\"WEEK\" = \"dengue$發病周\", \n         \"VILLCODE\" = \"tainan$VILLCODE\") %&gt;% \n  left_join(tainan, by = \"VILLCODE\") %&gt;% \n  select(\"WEEK\", \"VILLCODE\", \"geometry\") %&gt;% \n  st_as_sf(crs = \"WGS84\")\n\nspatial_temporal_dengue_count$DENGUE_COUNT &lt;- NA\n\nfor (t in 31:50) {\n  \n  spatial_temporal_dengue_count[spatial_temporal_dengue_count$WEEK == t, \"DENGUE_COUNT\"] &lt;- \n    st_intersects(\n      spatial_temporal_dengue_count %&gt;% \n        filter(WEEK == t), \n      dengue %&gt;% \n        filter(發病周 == t)\n      ) %&gt;% \n    lengths()\n  \n}\n\nspatial_temporal_dengue_count &lt;- spatial_temporal_dengue_count %&gt;% \n  st_drop_geometry()\n\nglimpse(spatial_temporal_dengue_count)\nsum(spatial_temporal_dengue_count$DENGUE_COUNT)\n\n\ncrossing() is used to create a dataframe of all possible weeks over all spatial regions in Tainan, resulting in a total of 5160 combinations. After which, st_intersects() and filter() are employed iteratively, to give a count of the number of dengue reports in each region over each time period.\n\n\nReveal Code\nspatial_temporal_dengue_count %&gt;% \n  write_rds(\"Data/RDS/spatial_temporal_dengue_count.rds\")\n\n\n\n\nRows: 5,160\nColumns: 3\n$ WEEK         &lt;dbl&gt; 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 3…\n$ VILLCODE     &lt;chr&gt; \"67000270001\", \"67000270002\", \"67000270003\", \"67000270004…\n$ DENGUE_COUNT &lt;int&gt; 0, 0, 1, 4, 2, 0, 1, 0, 1, 7, 1, 2, 0, 0, 0, 0, 42, 0, 0,…\n\n\n[1] 18800\n\n\nA sum() of our counts column matches up with the total number of cases in our reduced dengue_daily data.\n\n\nReveal Code\ndengue_spt &lt;- spacetime(.data = spatial_temporal_dengue_count, \n                        .geometry = tainan,\n                        .loc_col = \"VILLCODE\", \n                        .time_col = \"WEEK\")\n\nis_spacetime_cube(dengue_spt)\n\n\n[1] TRUE\n\n\nAnd is_spacetime_cube() confirms that we were successful. Thus, we can proceed to conduct the emerging hot-spot analysis using our spacetime cube, upon the DENGUE_COUNT feature which we created in the earlier step. The analysis is performed using the emerging_hotspot_analysis() function from sfdep, which utilises both Gerd-Ord’s Gi* statistic and Mann-Kendall statistic to do so. Thus, Kendall needs to be installed in the R-Environment as earlier.\n\n\nReveal Code\nehsa &lt;- emerging_hotspot_analysis(\n  x = dengue_spt, \n  .var = \"DENGUE_COUNT\", \n  k = 1, \n  nsim = 499)\n\nglimpse(ehsa)\n\n\n\n\nReveal Code\nehsa %&gt;% \n  write_rds(\"Data/RDS/dengue_ehsa.rds\")\n\n\n\n\nRows: 258\nColumns: 4\n$ location       &lt;chr&gt; \"67000350032\", \"67000270011\", \"67000370005\", \"670003300…\n$ tau            &lt;dbl&gt; 0.37894735, 0.36842102, 0.56842101, 0.41052628, 0.54736…\n$ p_value        &lt;dbl&gt; 2.124822e-02, 2.517831e-02, 5.174875e-04, 1.248217e-02,…\n$ classification &lt;chr&gt; \"oscilating coldspot\", \"oscilating coldspot\", \"consecut…\n\n\nUsing ggplot2, we can visualise the frequency of each type of hot-spot identified in our emerging hot-spot analysis Monte-Carlo simulation.\n\n\nReveal Code\nggplot(data = ehsa,\n       aes(x = classification)) +\n  geom_bar(fill = \"orange\") + \n  labs(title = \"Types of Hot-Spots Identified\") +\n  xlab(\"Classification\") +\n  ylab(\"Number of hot-spots\") +\n  theme(axis.text.x = element_text(angle = 10, hjust = 0.5, vjust = 0.7),\n        panel.background = element_rect(fill = \"lightgrey\")) \n\n\n\n\n\n\n\n\n\nAnd using tmap, we can display an interactive map of the different hot-spots over OpenStreetMap.\n\n\nReveal Code\nehsa_leaflet &lt;- tainan %&gt;% \n  left_join(ehsa,\n            by = join_by(VILLCODE == location)) %&gt;% \n  tm_shape(filter = .$p_value &lt;= 0.05) +\n    tm_fill(\"classification\",\n            interactive = TRUE,\n            alpha = 0.3,\n            title = \"Classification\") + \n    tm_borders(alpha = 1) + \n  tm_basemap(\"OpenStreetMap\")\n\nehsa_leaflet &lt;- tmap_leaflet(ehsa_leaflet)\n\nsaveWidget(ehsa_leaflet, \"Screenshots/ehsa_leaflet.html\")\n\n\n\n\n\n\nFrom the interactive map produced, we can see that most of the hot-spots are oscillating hot and cold spots, and are also mostly centered around the central parts of our map scope. There is actually quite an intuitive explanation. The proximity and human density of the hot/cold spots are probably really high considering their location, which will only make it easier for Dengue Fever to spread. Additionally, public facilities like pipes and drains are likely to be under higher stress, making it easier for Aedes mosquitoes to breed. Thus, it is likely that these areas will be most prone to dengue outbreaks and it is a testament to Taiwan’s dengue management that they are oscillation spots, because this mostly likely happens as they are able to contain outbreaks but not fast enough to stop the spread to other regions."
  }
]